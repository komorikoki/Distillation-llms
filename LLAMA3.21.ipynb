{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"teacher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1]], device='cuda:0') {'input_ids': tensor([[128000,  15546,    527,    499,     30]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "tensor([[128000,  15546,    527,    499,     30,    264,    220,    220,    220,\n",
      "            220,     16,    279,    315,    311,    315,    315,   1174,    315,\n",
      "            220,    220,    220,     20,    220,    220,    571,     31,    571,\n",
      "             31,   1174,    220,     31,    220,     31,   1174,    279,   1174,\n",
      "            279,   1174,    315,    315,    315,    220,     31,    279,    662,\n",
      "            220,    220,   1174,    311,   1174,   1174,    279,    279,   1174,\n",
      "            220,    220,    220,    220,     31,    279,    304,    279,    304,\n",
      "            315,    311,    662, 128009]], device='cuda:0')\n",
      "Who are you? a    1 the of to of of, of   5   @@ @@, @ @, the, the, of of of @ the.  , to,, the the,    @ the in the in of to.\n"
     ]
    }
   ],
   "source": [
    "# 質問の入力\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "device='cuda'\n",
    "question = \"Who are you?\"\n",
    "input_ids = tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "model.to(device)\n",
    "\n",
    "attention_mask=input_ids['attention_mask']\n",
    "print(attention_mask, input_ids)\n",
    "\n",
    "output = model.generate(input_ids['input_ids'], attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id, max_length=100)\n",
    "print(output)\n",
    "\n",
    "# 出力のデコード\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 128256])\n"
     ]
    }
   ],
   "source": [
    "question = \"b b b\"\n",
    "input_ids = tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "output = model(input_ids['input_ids'], attention_mask=attention_mask)\n",
    "print(output.logits.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.4916e-06, 7.2691e-06, 2.9318e-06,  ..., 1.1749e-07, 1.1751e-07,\n",
      "        1.1748e-07], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor(220, device='cuda:0')\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "probabilities = torch.softmax(output.logits[0][2], dim=-1)\n",
    "print(probabilities)\n",
    "predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "print(predicted_token_ids)\n",
    "decoded_text = tokenizer.decode(predicted_token_ids, skip_special_tokens=True)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": [\n",
       "    128001,\n",
       "    128008,\n",
       "    128009\n",
       "  ],\n",
       "  \"temperature\": 0.6,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForCausalLM                                        --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   262,668,288\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-5                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-6                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-7                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-8                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-9                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-10                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-11                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-12                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-13                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-14                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-15                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-16                     60,821,504\n",
       "│    └─LlamaRMSNorm: 2-3                                2,048\n",
       "│    └─LlamaRotaryEmbedding: 2-4                        --\n",
       "├─Linear: 1-2                                           262,668,288\n",
       "================================================================================\n",
       "Total params: 1,498,482,688\n",
       "Trainable params: 1,498,482,688\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distillLLAMA\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distillLLAMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForCausalLM                                        --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   262,668,288\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-5                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-6                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-7                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-8                      60,821,504\n",
       "│    └─LlamaRMSNorm: 2-3                                2,048\n",
       "│    └─LlamaRotaryEmbedding: 2-4                        --\n",
       "├─Linear: 1-2                                           262,668,288\n",
       "================================================================================\n",
       "Total params: 1,011,910,656\n",
       "Trainable params: 1,011,910,656\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/my-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForCausalLM                                        --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   262,668,288\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      60,821,504\n",
       "│    └─LlamaRMSNorm: 2-3                                2,048\n",
       "│    └─LlamaRotaryEmbedding: 2-4                        --\n",
       "├─Linear: 1-2                                           262,668,288\n",
       "================================================================================\n",
       "Total params: 768,624,640\n",
       "Trainable params: 768,624,640\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distillLLAMA2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distillLLAMA2\")\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(model, depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
