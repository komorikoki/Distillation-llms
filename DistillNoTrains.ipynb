{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# OpenWebTextデータセットのロード\n",
    "dataset = load_dataset('openwebtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 'train'セットから5000サンプルを取得\n",
    "limited_dataset = dataset['train'].select(range(5000))\n",
    "\n",
    "# データの確認\n",
    "print(limited_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "# トークナイザーをロード\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "\n",
    "# GPT-2の設定を作成（小型モデル用）\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ") # \"gpt2\"を指定して小型モデルを取得\n",
    "# モデルを初期化\n",
    "student_model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in student_model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 774.0M parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# デバイスの設定\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# トークナイザーと教師モデルをロード\n",
    "teacher_model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
    "model_size = sum(t.numel() for t in teacher_model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "teacher_model.to(device)  # GPUを使用する場合\n",
    "student_model.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def distillation_loss(student_logits, teacher_logits, temperature=2.0):\n",
    "    teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "    student_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
    "    return F.kl_div(student_probs, teacher_probs) * (temperature ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Progress:   0%|          | 0/5000 [00:00<?, ?it/s]/root/miniconda3/envs/my-env/lib/python3.9/site-packages/torch/nn/functional.py:3369: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 9.94927468127571e-05\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 8.963559230323881e-05\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 8.472816261928529e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# オプティマイザーの設定\n",
    "optimizer = optim.AdamW(student_model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 3  # エポック数\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    epoch_progress = tqdm(limited_dataset, desc=\"Batch Progress\", leave=False)\n",
    "    \n",
    "    for example in epoch_progress:\n",
    "        input_text = example['text']\n",
    "        inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True).to('cuda')\n",
    "\n",
    "        # 教師モデルの出力を取得\n",
    "        with torch.no_grad():\n",
    "            teacher_output = teacher_model(**inputs)\n",
    "            teacher_logits = teacher_output.logits\n",
    "\n",
    "        # 生徒モデルの出力を取得\n",
    "        student_output = student_model(**inputs)\n",
    "        student_logits = student_output.logits\n",
    "\n",
    "        # 蒸留損失の計算\n",
    "        loss = distillation_loss(student_logits, teacher_logits)\n",
    "        \n",
    "        # 勾配計算とオプティマイザーのステップ\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 各バッチの損失を進捗バーに表示\n",
    "        epoch_progress.set_postfix({\"Loss\": loss.item()})\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Model - Loss: 3.3081, Perplexity: 33.6072\n",
      "Student Model - Loss: 5.7662, Perplexity: 352.4743\n"
     ]
    }
   ],
   "source": [
    "# 評価関数の定義\n",
    "def evaluate_model(model, tokenizer, input_texts, device):\n",
    "    model.eval()  # 評価モード\n",
    "    total_loss = 0\n",
    "    total_perplexity = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for text in input_texts:\n",
    "            # 入力をデバイスに移動\n",
    "            inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])  # 入力をモデルに渡す\n",
    "            loss = outputs.loss.item()\n",
    "            total_loss += loss\n",
    "            perplexity = torch.exp(torch.tensor(loss))\n",
    "            total_perplexity += perplexity.item()\n",
    "            count += 1\n",
    "\n",
    "    # 平均損失とパープレキシティを計算\n",
    "    avg_loss = total_loss / count if count > 0 else float('inf')\n",
    "    avg_perplexity = total_perplexity / count if count > 0 else float('inf')\n",
    "    return avg_loss, avg_perplexity\n",
    "\n",
    "# 使用例\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 評価データを定義\n",
    "input_texts = [\"This is a test sentence.\", \"How are you today?\"]\n",
    "\n",
    "# 教師モデルの評価\n",
    "teacher_loss, teacher_perplexity = evaluate_model(teacher_model, tokenizer, input_texts, device)\n",
    "print(f\"Teacher Model - Loss: {teacher_loss:.4f}, Perplexity: {teacher_perplexity:.4f}\")\n",
    "\n",
    "# 生徒モデルの評価\n",
    "student_loss, student_perplexity = evaluate_model(student_model, tokenizer, input_texts, device)\n",
    "print(f\"Student Model - Loss: {student_loss:.4f}, Perplexity: {student_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.save_pretrained('distill_GPTlm_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ") # \"gpt2\"を指定して小型モデルを取得\n",
    "# モデルを初期化\n",
    "no_train_model = GPT2LMHeadModel(config)\n",
    "\n",
    "no_train_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No train Model - Loss: 11.2473, Perplexity: 78959.7949\n"
     ]
    }
   ],
   "source": [
    "no_train_loss, no_train_perplexity = evaluate_model(no_train_model, tokenizer, input_texts, device)\n",
    "print(f\"No train Model - Loss: {no_train_loss:.4f}, Perplexity: {no_train_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
