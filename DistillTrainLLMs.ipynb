{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "# トークナイザーをロード\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# OpenWebTextデータセットのロード\n",
    "dataset = load_dataset('openwebtext')\n",
    "\n",
    "# 'train'セットから5000サンプルを取得\n",
    "limited_dataset = dataset['train'].select(range(5000))\n",
    "\n",
    "print(limited_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "\n",
    "# GPT-2の設定を作成（小型モデル用）\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ") # \"gpt2\"を指定して小型モデルを取得\n",
    "# モデルを初期化\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.0005087573081254959\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# 各バッチの損失を進捗バーに表示\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     epoch_progress\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m})\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# オプティマイザーの設定\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "model.train()  \n",
    "\n",
    "num_epochs = 3  # エポック数\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    epoch_progress = tqdm(limited_dataset, desc=\"Batch Progress\", leave=False)\n",
    "    \n",
    "    for example in epoch_progress:\n",
    "        input_text = example['text']\n",
    "        inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True).to('cuda')\n",
    "\n",
    "        output = model(**inputs)\n",
    "        logits = output.logits\n",
    "\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), inputs[\"input_ids\"].view(-1).to('cuda'))\n",
    "        \n",
    "        # 勾配計算とオプティマイザーのステップ\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 各バッチの損失を進捗バーに表示\n",
    "        epoch_progress.set_postfix({\"Loss\": loss.item()})\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model - Loss: 22.4715, Perplexity: 7314218112.0000\n"
     ]
    }
   ],
   "source": [
    "# 評価関数の定義\n",
    "def evaluate_model(model, tokenizer, input_texts, device):\n",
    "    model.eval()  # 評価モード\n",
    "    total_loss = 0\n",
    "    total_perplexity = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for text in input_texts:\n",
    "            inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])  # 入力をモデルに渡す\n",
    "            loss = outputs.loss.item()\n",
    "            total_loss += loss\n",
    "            perplexity = torch.exp(torch.tensor(loss))\n",
    "            total_perplexity += perplexity.item()\n",
    "            count += 1\n",
    "\n",
    "    # 平均損失とパープレキシティを計算\n",
    "    avg_loss = total_loss / count if count > 0 else float('inf')\n",
    "    avg_perplexity = total_perplexity / count if count > 0 else float('inf')\n",
    "    return avg_loss, avg_perplexity\n",
    "\n",
    "# 使用例\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 評価データを定義\n",
    "input_texts = [\"This is a test sentence.\", \"How are you today?\"]\n",
    "\n",
    "# 教師モデルの評価\n",
    "loss, perplexity = evaluate_model(model, tokenizer, input_texts, device)\n",
    "print(f\"Model - Loss: {loss:.4f}, Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 474.71 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = './distill_GPTlm_model/model.safetensors'\n",
    "file_size = os.path.getsize(file_path)  # ファイルサイズをバイトで取得\n",
    "print(f\"File size: {file_size / (1024 * 1024):.2f} MB\")  # MB単位で表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
