{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DistilBertForMaskedLM                                   --\n",
       "├─GELUActivation: 1-1                                   --\n",
       "├─DistilBertModel: 1-2                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              23,440,896\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─LayerNorm: 3-3                              1,536\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             --\n",
       "│    │    │    └─TransformerBlock: 4-1                  --\n",
       "│    │    │    │    └─MultiHeadSelfAttention: 5-1       --\n",
       "│    │    │    │    │    └─Dropout: 6-1                 --\n",
       "│    │    │    │    │    └─Linear: 6-2                  590,592\n",
       "│    │    │    │    │    └─Linear: 6-3                  590,592\n",
       "│    │    │    │    │    └─Linear: 6-4                  590,592\n",
       "│    │    │    │    │    └─Linear: 6-5                  590,592\n",
       "│    │    │    │    └─LayerNorm: 5-2                    1,536\n",
       "│    │    │    │    └─FFN: 5-3                          --\n",
       "│    │    │    │    │    └─Dropout: 6-6                 --\n",
       "│    │    │    │    │    └─Linear: 6-7                  2,362,368\n",
       "│    │    │    │    │    └─Linear: 6-8                  2,360,064\n",
       "│    │    │    │    │    └─GELUActivation: 6-9          --\n",
       "│    │    │    │    └─LayerNorm: 5-4                    1,536\n",
       "│    │    │    └─TransformerBlock: 4-2                  --\n",
       "│    │    │    │    └─MultiHeadSelfAttention: 5-5       --\n",
       "│    │    │    │    │    └─Dropout: 6-10                --\n",
       "│    │    │    │    │    └─Linear: 6-11                 590,592\n",
       "│    │    │    │    │    └─Linear: 6-12                 590,592\n",
       "│    │    │    │    │    └─Linear: 6-13                 590,592\n",
       "│    │    │    │    │    └─Linear: 6-14                 590,592\n",
       "│    │    │    │    └─LayerNorm: 5-6                    1,536\n",
       "│    │    │    │    └─FFN: 5-7                          --\n",
       "│    │    │    │    │    └─Dropout: 6-15                --\n",
       "│    │    │    │    │    └─Linear: 6-16                 2,362,368\n",
       "│    │    │    │    │    └─Linear: 6-17                 2,360,064\n",
       "│    │    │    │    │    └─GELUActivation: 6-18         --\n",
       "│    │    │    │    └─LayerNorm: 5-8                    1,536\n",
       "│    │    │    └─TransformerBlock: 4-3                  --\n",
       "│    │    │    │    └─MultiHeadSelfAttention: 5-9       --\n",
       "│    │    │    │    │    └─Dropout: 6-19                --\n",
       "│    │    │    │    │    └─Linear: 6-20                 590,592\n",
       "│    │    │    │    │    └─Linear: 6-21                 590,592\n",
       "│    │    │    │    │    └─Linear: 6-22                 590,592\n",
       "│    │    │    │    │    └─Linear: 6-23                 590,592\n",
       "│    │    │    │    └─LayerNorm: 5-10                   1,536\n",
       "│    │    │    │    └─FFN: 5-11                         --\n",
       "│    │    │    │    │    └─Dropout: 6-24                --\n",
       "│    │    │    │    │    └─Linear: 6-25                 2,362,368\n",
       "│    │    │    │    │    └─Linear: 6-26                 2,360,064\n",
       "│    │    │    │    │    └─GELUActivation: 6-27         --\n",
       "│    │    │    │    └─LayerNorm: 5-12                   1,536\n",
       "│    │    │    └─TransformerBlock: 4-4                  --\n",
       "│    │    │    │    └─MultiHeadSelfAttention: 5-13      --\n",
       "│    │    │    │    │    └─Dropout: 6-28                --\n",
       "│    │    │    │    │    └─Linear: 6-29                 590,592\n",
       "│    │    │    │    │    └─Linear: 6-30                 590,592\n",
       "│    │    │    │    │    └─Linear: 6-31                 590,592\n",
       "│    │    │    │    │    └─Linear: 6-32                 590,592\n",
       "│    │    │    │    └─LayerNorm: 5-14                   1,536\n",
       "│    │    │    │    └─FFN: 5-15                         --\n",
       "│    │    │    │    │    └─Dropout: 6-33                --\n",
       "│    │    │    │    │    └─Linear: 6-34                 2,362,368\n",
       "│    │    │    │    │    └─Linear: 6-35                 2,360,064\n",
       "│    │    │    │    │    └─GELUActivation: 6-36         --\n",
       "│    │    │    │    └─LayerNorm: 5-16                   1,536\n",
       "│    │    │    └─TransformerBlock: 4-5                  --\n",
       "│    │    │    │    └─MultiHeadSelfAttention: 5-17      --\n",
       "│    │    │    │    │    └─Dropout: 6-37                --\n",
       "│    │    │    │    │    └─Linear: 6-38                 590,592\n",
       "│    │    │    │    │    └─Linear: 6-39                 590,592\n",
       "│    │    │    │    │    └─Linear: 6-40                 590,592\n",
       "│    │    │    │    │    └─Linear: 6-41                 590,592\n",
       "│    │    │    │    └─LayerNorm: 5-18                   1,536\n",
       "│    │    │    │    └─FFN: 5-19                         --\n",
       "│    │    │    │    │    └─Dropout: 6-42                --\n",
       "│    │    │    │    │    └─Linear: 6-43                 2,362,368\n",
       "│    │    │    │    │    └─Linear: 6-44                 2,360,064\n",
       "│    │    │    │    │    └─GELUActivation: 6-45         --\n",
       "│    │    │    │    └─LayerNorm: 5-20                   1,536\n",
       "│    │    │    └─TransformerBlock: 4-6                  --\n",
       "│    │    │    │    └─MultiHeadSelfAttention: 5-21      --\n",
       "│    │    │    │    │    └─Dropout: 6-46                --\n",
       "│    │    │    │    │    └─Linear: 6-47                 590,592\n",
       "│    │    │    │    │    └─Linear: 6-48                 590,592\n",
       "│    │    │    │    │    └─Linear: 6-49                 590,592\n",
       "│    │    │    │    │    └─Linear: 6-50                 590,592\n",
       "│    │    │    │    └─LayerNorm: 5-22                   1,536\n",
       "│    │    │    │    └─FFN: 5-23                         --\n",
       "│    │    │    │    │    └─Dropout: 6-51                --\n",
       "│    │    │    │    │    └─Linear: 6-52                 2,362,368\n",
       "│    │    │    │    │    └─Linear: 6-53                 2,360,064\n",
       "│    │    │    │    │    └─GELUActivation: 6-54         --\n",
       "│    │    │    │    └─LayerNorm: 5-24                   1,536\n",
       "├─Linear: 1-3                                           590,592\n",
       "├─LayerNorm: 1-4                                        1,536\n",
       "├─Linear: 1-5                                           23,471,418\n",
       "├─CrossEntropyLoss: 1-6                                 --\n",
       "================================================================================\n",
       "Total params: 90,426,426\n",
       "Trainable params: 90,426,426\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 質問の入力\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "device='cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a great [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2023, 2003, 1037, 2307,  103, 1012,  102]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device='cuda'\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "model.to(device)\n",
    "token_logits = model(**inputs).logits\n",
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
