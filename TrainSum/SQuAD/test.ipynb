{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "device='cuda'\n",
    "ds = load_dataset(\"rajpurkar/squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained('../model/normal_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 100\n",
    "data_size_v = 100\n",
    "size = int(data_size/4)\n",
    "size_v = int(data_size_v/4)\n",
    "train_dataset=ds[\"train\"].shuffle(seed=42).select(range(100))\n",
    "validation_dataset=ds[\"validation\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(dataset):\n",
    "    reshape_dataset = [0] * len(dataset)\n",
    "    for i in range(len(dataset)):\n",
    "        reshape_dataset[i]=\"C: \"+dataset[i][\"context\"]+\" Q: \"+dataset[i][\"question\"]+\" A: \"+dataset[i][\"answers\"][\"text\"][0]\n",
    "    reshape_dataset = [item for item in reshape_dataset if item != '' and len(item) >= 50 and '@' not in item]\n",
    "    reshape_dataset = [re.sub(r'[^a-zA-Z0-9 .:?]', '', item) for item in reshape_dataset]\n",
    "    reshape_dataset = [re.sub(r'\\s+', ' ', item) for item in reshape_dataset]\n",
    "    return reshape_dataset[:data_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(data):\n",
    "    dataset=reshape(data)\n",
    "    data = []\n",
    "    for text in tqdm(dataset, desc=\"Tokenizing dataset\"):\n",
    "        cq_len=len(tokenizer(text[:text.find(\"A:\")])['input_ids'])\n",
    "        tokenized = tokenizer(text, padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "        input_ids = tokenized['input_ids'].squeeze().tolist()\n",
    "        attention_mask = tokenized['attention_mask'].squeeze().tolist()\n",
    "        labels = input_ids[1:] + [tokenizer.pad_token_id]\n",
    "        for i in range(min(cq_len-2, 512)):\n",
    "            labels[i]=128001\n",
    "        data.append({\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\":attention_mask})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset: 100%|██████████| 100/100 [00:00<00:00, 1346.89it/s]\n",
      "Tokenizing dataset: 100%|██████████| 100/100 [00:00<00:00, 1338.46it/s]\n"
     ]
    }
   ],
   "source": [
    "data = make_data(train_dataset)\n",
    "data_v = make_data(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsdata=[]\n",
    "for i in range(data_size):\n",
    "    data_l = [x for x in data[i]['labels'] if x != 128001]\n",
    "    labelsdata.append(data_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[362, 25, 220, 5833]\n",
      " A: 84\n",
      "[362, 25, 6603]\n",
      " A: books\n",
      "[362, 25, 279, 11145]\n",
      " A: the executive\n",
      "[362, 25, 1556, 7910, 299]\n",
      " A: Anjiro\n",
      "[362, 25, 30853]\n",
      " A: loops\n",
      "[362, 25, 220, 17, 13, 17, 7239]\n",
      " A: 2.2 billion\n",
      "[362, 25, 28058, 24245, 315, 279, 549, 815, 13, 99452, 22967]\n",
      " A: Military Governor of the U.S. Occupation Zone\n",
      "[362, 25, 279, 14198, 3026]\n",
      " A: the brown men\n",
      "[362, 25, 5070, 1436, 387, 17550, 311, 279, 10977, 520, 12474, 5326]\n",
      " A: resources could be targeted to the communities at greatest risk\n",
      "[362, 25, 26828, 61495]\n",
      " A: honey ants\n",
      "[362, 25, 578, 356, 3746, 7977]\n",
      " A: The Cossacks\n",
      "[362, 25, 26742, 5346, 285]\n",
      " A: verdigris\n",
      "[362, 25, 220, 4468, 15, 82]\n",
      " A: 1970s\n",
      "[362, 25, 8219, 55551, 329, 647]\n",
      " A: Sun Jiadong\n",
      "[362, 25, 4783, 11060]\n",
      " A: House Master\n",
      "[362, 25, 11888]\n",
      " A: nine\n",
      "[362, 25, 384, 14946, 324, 598, 323, 47715, 1371, 51835]\n",
      " A: echiurans and sipunculan\n",
      "[362, 25, 44193]\n",
      " A: Religion\n",
      "[362, 25, 1370, 5893, 32893, 24569]\n",
      " A: paralyzes muscles\n",
      "[362, 25, 8305, 24520]\n",
      " A: tituli\n",
      "[362, 25, 24991, 41445, 4409]\n",
      " A: CBS Television City\n",
      "[362, 25, 220, 10861, 15, 323, 220, 11908, 19]\n",
      " A: 1720 and 1734\n",
      "[362, 25, 220, 6889, 17168, 24]\n",
      " A: 1032949\n",
      "[362, 25, 279, 2380, 28711, 315, 42108, 11003, 27509, 82]\n",
      " A: the three grades of medieval craft guilds\n",
      "[362, 25, 1901, 383, 64647]\n",
      " A: Zhejiang\n",
      "[362, 25, 927, 220, 2031]\n",
      " A: over 70\n",
      "[362, 25, 6063, 21424, 28331]\n",
      " A: German Football Federation\n",
      "[362, 25, 6457]\n",
      " A: Japan\n",
      "[362, 25, 220, 3965, 311, 220, 3101, 9653]\n",
      " A: 150 to 300 mm\n",
      "[362, 25, 220, 1049, 18]\n",
      " A: 2003\n",
      "[362, 25, 1920, 90602, 5493, 527, 779, 17226, 323, 1380, 71200, 304, 872, 6325, 323, 12034]\n",
      " A: process theologians are so diverse and transdisciplinary in their views and interests\n",
      "[362, 25, 2225, 23354, 323, 67233]\n",
      " A: both popularity and skepticism\n",
      "[362, 25, 36375, 220, 17]\n",
      " A: exceeds 2\n",
      "[362, 25, 220, 10967, 18]\n",
      " A: 1763\n",
      "[362, 25, 4488, 43048, 16501]\n",
      " A: Mark Dybul\n",
      "[362, 25, 5659, 520, 3325, 279, 3389, 66089, 9478]\n",
      " A: From at least the late nineteenth century\n",
      "[362, 25, 65690, 81296, 304, 88012, 13011, 12302, 682, 2380, 315, 1202, 3682, 3313, 22019, 4101, 304, 9784, 520, 88012, 7327, 86749, 304, 7552]\n",
      " A: NASCAR headquartered in Daytona Beach begins all three of its major auto racing series in Florida at Daytona International Speedway in February\n",
      "[362, 25, 2522, 31529]\n",
      " A: Scuba\n",
      "[362, 25, 41329]\n",
      " A: sustainability\n",
      "[362, 25, 10384]\n",
      " A: Africa\n",
      "[362, 25, 452, 34858]\n",
      " A: Nasser\n",
      "[362, 25, 220, 1272]\n",
      " A: 40\n",
      "[362, 25, 12131, 47809]\n",
      " A: Richard Owen\n",
      "[362, 25, 8753]\n",
      " A: French\n",
      "[362, 25, 56979, 655]\n",
      " A: gabber\n",
      "[362, 25, 1825, 4319, 84, 1639]\n",
      " A: open circuited\n",
      "[362, 25, 6342, 18063, 58333]\n",
      " A: King Henry VIII\n",
      "[362, 25, 220, 18, 13, 16]\n",
      " A: 3.1\n",
      "[362, 25, 11234, 9799]\n",
      " A: Angevin\n",
      "[362, 25, 9564, 8771, 36767, 25844, 6675, 2009, 28927, 4852, 83, 304, 6460, 2394, 29578, 311, 7771, 2403, 8690, 30215, 505, 279, 11226]\n",
      " A: leaving Field Marshal Hans von Lehwaldt in East Prussia to guard against Russian invasion from the east\n",
      "[362, 25, 1884, 33142, 6646, 872, 57978, 4173, 437, 311, 387, 473, 16046, 292]\n",
      " A: those educated considered their ethnicity genos to be Hellenic\n",
      "[362, 25, 220, 9795]\n",
      " A: 136\n",
      "[362, 25, 7553, 60762]\n",
      " A: MacArthur\n",
      "[362, 25, 220, 1032, 5887, 220, 3753, 20]\n",
      " A: 13 July 1985\n",
      "[362, 25, 8013, 13865, 311, 27365, 420, 12108, 8246, 1051, 46025]\n",
      " A: British attempts to halt this fort construction were unsuccessful\n",
      "[362, 25, 220, 679, 15]\n",
      " A: 2010\n",
      "[362, 25, 15155]\n",
      " A: Italian\n",
      "[362, 25, 220, 717, 13, 1691, 3610]\n",
      " A: 12.21 million\n",
      "[362, 25, 220, 6393, 23]\n",
      " A: 1948\n",
      "[362, 25, 220, 3753, 22]\n",
      " A: 1987\n",
      "[362, 25, 220, 1490]\n",
      " A: 80\n",
      "[362, 25, 279, 5950, 8761, 12912]\n",
      " A: the grid street patterns\n",
      "[362, 25, 4379, 323, 586, 4375, 26280]\n",
      " A: police and public works departments\n",
      "[362, 25, 17422, 477, 2536, 58162]\n",
      " A: fiction or nonfiction\n",
      "[362, 25, 13537, 1990, 2204, 20314, 3600]\n",
      " A: connections between different electrical services\n",
      "[362, 25, 3297]\n",
      " A: May\n",
      "[362, 25, 304, 264, 3254, 3335]\n",
      " A: in a single record\n",
      "[362, 25, 11104]\n",
      " A: Western\n",
      "[362, 25, 6890]\n",
      " A: India\n",
      "[362, 25, 43059, 301]\n",
      " A: Sahel\n",
      "[362, 25, 2410, 8670]\n",
      " A: power requirements\n",
      "[362, 25, 12688]\n",
      " A: winter\n",
      "[362, 25, 5929, 4783, 315, 279, 39144, 2826]\n",
      " A: White House of the Confederacy\n",
      "[362, 25, 7984, 7082]\n",
      " A: Belgrade\n",
      "[362, 25, 10058, 650]\n",
      " A: George V\n",
      "[362, 25, 7600, 3750, 323, 23347]\n",
      " A: obscurity and mystery\n",
      "[362, 25, 22519, 323, 21648]\n",
      " A: pride and shame\n",
      "[362, 25, 15852, 18191]\n",
      " A: Magic Mouse\n",
      "[362, 25, 66102, 16549]\n",
      " A: Archaeologist\n",
      "[362, 25, 279, 10973, 10845, 30148, 1912]\n",
      " A: the absolute Galois group\n",
      "[362, 25, 5625, 13865]\n",
      " A: convert attempts\n",
      "[362, 25, 85962, 75612, 18112]\n",
      " A: Sassanian architecture\n",
      "[362, 25, 10264, 4406, 788]\n",
      " A: ringporous\n",
      "[362, 25, 10973, 3878]\n",
      " A: absolute terms\n",
      "[362, 25, 617, 11102, 279, 2383]\n",
      " A: have broken the law\n",
      "[362, 25, 5906, 95687, 304, 813, 1176, 25549, 9269]\n",
      " A: misstatements in his first divorce trial\n",
      "[362, 25, 459, 51943]\n",
      " A: an altar\n",
      "[362, 25, 7904, 32749, 780]\n",
      " A: Indian Carnatic\n",
      "[362, 25, 17118, 9053]\n",
      " A: Native Americans\n",
      "[362, 25, 6136, 62186]\n",
      " A: plant toxins\n",
      "[362, 25, 279, 15448, 2473, 277, 96486]\n",
      " A: the Unteraar Glacier\n",
      "[362, 25, 21222, 8519, 58358]\n",
      " A: Xbox Video Marketplace\n",
      "[362, 25, 33225]\n",
      " A: Tampa\n",
      "[362, 25, 87544, 304, 5150, 488, 323, 2349]\n",
      " A: deficient in actuality and change\n",
      "[362, 25, 8254]\n",
      " A: seven\n",
      "[362, 25, 25012, 279, 98741, 23575, 6661, 1603, 279, 78605, 519, 7891]\n",
      " A: placing the aspiration modifier letter before the consonant symbol\n",
      "[362, 25, 810, 1109, 220, 1399]\n",
      " A: more than 60\n",
      "[362, 25, 36011, 279, 8681]\n",
      " A: Vladimir the Great\n",
      "[362, 25, 15356, 329, 358]\n",
      " A: Murad I\n",
      "[362, 25, 220, 4331]\n",
      " A: 53\n"
     ]
    }
   ],
   "source": [
    "for date_la in labelsdata:\n",
    "    print(date_la) \n",
    "    print(tokenizer.decode(date_la))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "datav=[0,0,0,0,0]\n",
    "datav2 = datav[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datav2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset: 100%|██████████| 100/100 [00:00<00:00, 952.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight: requires_grad=True\n",
      "model.layers.0.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.0.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.0.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.0.self_attn.k_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.0.self_attn.k_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.0.self_attn.k_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.0.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.0.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.0.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.0.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.0.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.0.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.0.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.0.input_layernorm.weight: requires_grad=True\n",
      "model.layers.0.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.1.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.1.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.1.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.1.self_attn.k_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.1.self_attn.k_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.1.self_attn.k_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.1.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.1.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.1.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.1.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.1.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.1.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.1.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.1.input_layernorm.weight: requires_grad=True\n",
      "model.layers.1.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.2.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.2.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.2.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.2.self_attn.k_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.2.self_attn.k_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.2.self_attn.k_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.2.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.2.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.2.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.2.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.2.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.2.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.2.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.2.input_layernorm.weight: requires_grad=True\n",
      "model.layers.2.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.3.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.3.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.3.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.3.self_attn.k_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.3.self_attn.k_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.3.self_attn.k_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.3.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.3.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.3.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.3.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.3.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.3.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.3.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.3.input_layernorm.weight: requires_grad=True\n",
      "model.layers.3.post_attention_layernorm.weight: requires_grad=True\n",
      "model.norm.weight: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "def reshape(dataset):\n",
    "    reshape_dataset = [0] * len(dataset)\n",
    "    for i in range(len(dataset)):\n",
    "        reshape_dataset[i]=\"C: \"+dataset[i][\"context\"]+\" Q: \"+dataset[i][\"question\"]+\" A: \"+dataset[i][\"answers\"][\"text\"][0]\n",
    "    reshape_dataset = [item for item in reshape_dataset if item != '' and len(item) >= 50 and '@' not in item]\n",
    "    reshape_dataset = [re.sub(r'[^a-zA-Z0-9 .:?]', '', item) for item in reshape_dataset]\n",
    "    reshape_dataset = [re.sub(r'\\s+', ' ', item) for item in reshape_dataset]\n",
    "    return reshape_dataset[:data_size]\n",
    "\n",
    "def max_length(dataset):\n",
    "    max_eval=0\n",
    "    for i in dataset:\n",
    "        max_eval = len(i) if len(i) > max_eval else max_eval\n",
    "    print(max_eval)\n",
    "    return\n",
    "\n",
    "def batch(input, size):\n",
    "    batch_train=[]\n",
    "    for i in range(size):\n",
    "        batch_input=[input[4*i+0], input[4*i+1], input[4*i+2], input[4*i+3]]\n",
    "        batch_train.append(batch_input)\n",
    "\n",
    "    return batch_train\n",
    "\n",
    "def make_data(data):\n",
    "    dataset=reshape(data)\n",
    "    data = []\n",
    "    for text in tqdm(dataset, desc=\"Tokenizing dataset\"):\n",
    "        if len(tokenizer(text)['input_ids']) <= 256:\n",
    "            cq_len=len(tokenizer(text[:text.find(\"A:\")])['input_ids'])\n",
    "            tokenized = tokenizer(text, padding=\"max_length\", max_length=256, truncation=True, return_tensors=\"pt\")\n",
    "            input_ids = tokenized['input_ids'].squeeze().tolist()\n",
    "            attention_mask = tokenized['attention_mask'].squeeze().tolist()\n",
    "            labels = input_ids[1:] + [tokenizer.pad_token_id]\n",
    "            for i in range(min(cq_len-2, 256)):\n",
    "                labels[i]=128001\n",
    "            data.append({\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\":attention_mask})\n",
    "    \n",
    "    return data\n",
    "\n",
    "def make_tensor(data, type, size):\n",
    "    tmp = [item[type] for item in data]\n",
    "    tmp = batch(tmp, size)\n",
    "    tensor=torch.tensor(tmp, dtype=torch.long)\n",
    "    return tensor\n",
    "\n",
    "ds = load_dataset(\"rajpurkar/squad\")\n",
    "device='cuda'\n",
    "student_model = AutoModelForCausalLM.from_pretrained(\"../model/LoRA_distill_model\")\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\"./model/teacher_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "for name, param in student_model.named_parameters():\n",
    "    if not 'base' in name:\n",
    "        param.requires_grad = True\n",
    "        # if 'self' in name:\n",
    "        #     param.requires_grad = True\n",
    "\n",
    "data_size = 100\n",
    "\n",
    "train_dataset=ds[\"train\"].shuffle(seed=42)\n",
    "\n",
    "data = make_data(train_dataset)\n",
    "size=int(len(data)/4)\n",
    "input_ids_tensor = make_tensor(data, \"input_ids\", size)\n",
    "labels_tensor = make_tensor(data, \"labels\", size)\n",
    "attention_mask_tensor = make_tensor(data, \"attention_mask\", size)\n",
    "\n",
    "\n",
    "vocab_size = student_model.config.vocab_size\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=128001)\n",
    "\n",
    "criterion.to(device)\n",
    "input_ids_tensor=input_ids_tensor.to(device)\n",
    "labels_tensor=labels_tensor.to(device)\n",
    "attention_mask_tensor=attention_mask_tensor.to(device)\n",
    "\n",
    "student_model.to(device)\n",
    "teacher_model.to(device)\n",
    "\n",
    "epochs = 3\n",
    "lr=5e-5\n",
    "temperature = 1\n",
    "\n",
    "student_model.train()\n",
    "teacher_model.eval()\n",
    "\n",
    "for name, param in student_model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.named_parameters at 0x7f7c317125f0>\n"
     ]
    }
   ],
   "source": [
    "print(student_model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0363, -1.0363, -1.2363])\n",
      "tensor([-0.0208, -0.0208,  0.0459])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0043)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "P = torch.Tensor([0.4, 0.4, 0.2])\n",
    "Q = torch.Tensor([0.333, 0.333, 0.333])\n",
    "\n",
    "student_prob=F.log_softmax(P, dim=-1)\n",
    "teacher_prob=F.softmax(Q, dim=-1)\n",
    "print(student_prob)\n",
    "kldiv_loss=F.kl_div(student_prob, teacher_prob, reduction=\"none\")\n",
    "print(kldiv_loss)\n",
    "kldiv_loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0060, -0.0460,  0.0606])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kldiv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0060, -0.0460,  0.0606])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0975)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "P = torch.Tensor([0.36, 0.48, 0.16])\n",
    "Q = torch.Tensor([0.333, 0.333, 0.333])\n",
    "\n",
    "# Pは確率分布としてそのまま使用し、Qに対してlogを取る\n",
    "teacher_prob = F.softmax(Q, dim=-1)\n",
    "\n",
    "# 温度スケーリングを適用\n",
    "temperature = 1\n",
    "student_prob = P / temperature  # Pはすでに確率分布\n",
    "student_prob = torch.log(student_prob)  # Pの対数を取る\n",
    "\n",
    "# KLダイバージェンスの計算\n",
    "kldiv_loss = F.kl_div(student_prob, teacher_prob, reduction=\"none\")\n",
    "\n",
    "print(kldiv_loss.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset: 100%|██████████| 100/100 [00:00<00:00, 1077.75it/s]\n",
      "Tokenizing dataset: 100%|██████████| 600/600 [00:00<00:00, 1132.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "import optuna\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import gc\n",
    "\n",
    "ds = load_dataset(\"rajpurkar/squad\")\n",
    "device='cuda'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "student_model = AutoModelForCausalLM.from_pretrained(\"../Distill/model/distill_model\")\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\"./model/teacher_model\")\n",
    "\n",
    "data_size = 100\n",
    "data_size_v = 600\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "\n",
    "def reshape(dataset, d_size):\n",
    "    reshape_dataset = [0] * len(dataset)\n",
    "    for i in range(len(dataset)):\n",
    "        reshape_dataset[i]=\"C: \"+dataset[i][\"context\"]+\" Q: \"+dataset[i][\"question\"]+\" A: \"+dataset[i][\"answers\"][\"text\"][0]\n",
    "    reshape_dataset = [item for item in reshape_dataset if item != '' and len(item) >= 50 and '@' not in item]\n",
    "    reshape_dataset = [re.sub(r'[^a-zA-Z0-9 .:?]', '', item) for item in reshape_dataset]\n",
    "    reshape_dataset = [re.sub(r'\\s+', ' ', item) for item in reshape_dataset]\n",
    "    return reshape_dataset[:d_size]\n",
    "\n",
    "def max_length(dataset):\n",
    "    max_eval=0\n",
    "    for i in dataset:\n",
    "        max_eval = len(i) if len(i) > max_eval else max_eval\n",
    "    print(max_eval)\n",
    "    return\n",
    "\n",
    "def batch(input, size):\n",
    "    batch_train=[]\n",
    "    for i in range(size):\n",
    "        batch_input=[input[4*i+0], input[4*i+1], input[4*i+2], input[4*i+3]]\n",
    "        batch_train.append(batch_input)\n",
    "\n",
    "    return batch_train\n",
    "\n",
    "def make_data(data, d_size):\n",
    "    dataset=reshape(data, d_size)\n",
    "    data = []\n",
    "    for text in tqdm(dataset, desc=\"Tokenizing dataset\"):\n",
    "        if len(tokenizer(text)['input_ids']) <= 256:\n",
    "            cq_len=len(tokenizer(text[:text.find(\"A:\")])['input_ids'])\n",
    "            tokenized = tokenizer(text, padding=\"max_length\", max_length=256, truncation=True, return_tensors=\"pt\")\n",
    "            input_ids = tokenized['input_ids'].squeeze().tolist()\n",
    "            attention_mask = tokenized['attention_mask'].squeeze().tolist()\n",
    "            labels = input_ids[1:] + [tokenizer.pad_token_id]\n",
    "            for i in range(min(cq_len-2, 256)):\n",
    "                labels[i]=128001\n",
    "            data.append({\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\":attention_mask})\n",
    "    \n",
    "    return data\n",
    "\n",
    "def make_tensor(data, type, size):\n",
    "    tmp = [item[type] for item in data]\n",
    "    tmp = batch(tmp, size)\n",
    "    tensor=torch.tensor(tmp, dtype=torch.long)\n",
    "    return tensor\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-7, 1e-4, log=True)\n",
    "    temperature = trial.suggest_float(\"temperature\", 1.0, 10.0)\n",
    "\n",
    "    student_model=AutoModelForCausalLM.from_pretrained(\"./model/optim_distilledmodel\")\n",
    "    student_model.to(device)\n",
    "    student_model.train()\n",
    "    optimizer = AdamW(student_model.parameters(), lr=lr)\n",
    "    losses = 0\n",
    "    for i in tqdm(range(size_v)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        student_logits = student_model(input_ids=input_ids_tensor_v[i], attention_mask=attention_mask_tensor_v[i]).logits.view(-1, vocab_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher_model(input_ids=input_ids_tensor_v[i], attention_mask=attention_mask_tensor_v[i]).logits.view(-1, vocab_size)\n",
    "        mask = labels_tensor[i].view(-1) != 128001\n",
    "        student_prob = F.log_softmax(student_logits[mask] / temperature, dim=-1)\n",
    "        teacher_prob = F.softmax(teacher_logits[mask] / temperature, dim=-1)\n",
    "\n",
    "        kl_loss = F.kl_div(student_prob, teacher_prob, reduction=\"none\").sum(dim=-1).sum()\n",
    "        kl_loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += kl_loss\n",
    "        \n",
    "    del student_logits, teacher_logits, student_prob, teacher_prob\n",
    "    return losses\n",
    "\n",
    "train_dataset=ds[\"train\"].shuffle(seed=42)\n",
    "validation_dataset = ds[\"validation\"].shuffle(seed=42)\n",
    "\n",
    "data = make_data(train_dataset, data_size)\n",
    "data_v = make_data(validation_dataset, data_size_v)\n",
    "size=int(len(data)/4)\n",
    "size_v=100\n",
    "\n",
    "input_ids_tensor = make_tensor(data, \"input_ids\", size)\n",
    "labels_tensor = make_tensor(data, \"labels\", size)\n",
    "attention_mask_tensor = make_tensor(data, \"attention_mask\", size)\n",
    "input_ids_tensor_v = make_tensor(data_v, \"input_ids\", size_v)\n",
    "labels_tensor_v = make_tensor(data_v, \"labels\", size_v)\n",
    "attention_mask_tensor_v = make_tensor(data_v, \"attention_mask\", size_v)\n",
    "\n",
    "vocab_size = teacher_model.config.vocab_size\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=128001)\n",
    "\n",
    "criterion.to(device)\n",
    "input_ids_tensor=input_ids_tensor.to(device)\n",
    "labels_tensor=labels_tensor.to(device)\n",
    "attention_mask_tensor=attention_mask_tensor.to(device)\n",
    "input_ids_tensor_v=input_ids_tensor_v.to(device)\n",
    "labels_tensor_v=labels_tensor_v.to(device)\n",
    "attention_mask_tensor_v=attention_mask_tensor_v.to(device)\n",
    "\n",
    "\n",
    "student_model.to(device)\n",
    "student_model.eval()\n",
    "\n",
    "student_logits = student_model(input_ids=input_ids_tensor[0], attention_mask=attention_mask_tensor[0]).logits.view(-1, vocab_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 128256])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_logits.view(-1, vocab_size).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
