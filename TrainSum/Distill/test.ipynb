{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\")\n",
    "device='cuda'\n",
    "# モデルの準備\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "student_model = AutoModelForCausalLM.from_pretrained(\"./model/initialized_distill_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128001\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForCausalLM                                        --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   262,668,288\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-5                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-6                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-7                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-8                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-9                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-10                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-11                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-12                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-13                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-14                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-15                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-16                     60,821,504\n",
       "│    └─LlamaRMSNorm: 2-3                                2,048\n",
       "│    └─LlamaRotaryEmbedding: 2-4                        --\n",
       "├─Linear: 1-2                                           262,668,288\n",
       "================================================================================\n",
       "Total params: 1,498,482,688\n",
       "Trainable params: 1,498,482,688\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(teacher_model, depth=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForCausalLM                                        --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   262,668,288\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-5                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-6                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-7                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-8                      60,821,504\n",
       "│    └─LlamaRMSNorm: 2-3                                2,048\n",
       "│    └─LlamaRotaryEmbedding: 2-4                        --\n",
       "├─Linear: 1-2                                           262,668,288\n",
       "================================================================================\n",
       "Total params: 1,011,910,656\n",
       "Trainable params: 1,011,910,656\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(student_model, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n",
      "1456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset: 100%|██████████| 100/100 [00:00<00:00, 2665.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.6776, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\")\n",
    "device='cuda'\n",
    "# モデルの準備\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"../model/initialized_distill_model2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "data_size = 100\n",
    "size = int(data_size/4)\n",
    "\n",
    "train_dataset=ds[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "def reshape(dataset):\n",
    "    dataset=dataset[\"text\"]\n",
    "    dataset = [item for item in dataset if item != '' and len(item) >= 50 and '@' not in item]\n",
    "    dataset = [re.sub(r'[^a-zA-Z0-9 ?]', '', item) for item in dataset]\n",
    "    dataset = [re.sub(r'\\s+', ' ', item) for item in dataset]\n",
    "    print(len(dataset))\n",
    "    return dataset[:data_size]\n",
    "\n",
    "def max_length(dataset):\n",
    "    max_eval=0\n",
    "    for i in dataset:\n",
    "        max_eval = len(i) if len(i) > max_eval else max_eval\n",
    "    print(max_eval)\n",
    "    return\n",
    "\n",
    "\n",
    "dataset=reshape(train_dataset)\n",
    "max_length(dataset)\n",
    "\n",
    "def batch(input):\n",
    "    batch_train=[]\n",
    "    for i in range(size):\n",
    "        batch_input=[input[4*i+0], input[4*i+1], input[4*i+2], input[4*i+3]]\n",
    "        batch_train.append(batch_input)\n",
    "\n",
    "    return batch_train\n",
    "\n",
    "# 入力とラベルを設定\n",
    "data = []\n",
    "for text in tqdm(dataset, desc=\"Tokenizing dataset\"):\n",
    "    tokenized = tokenizer(text, padding=\"max_length\", max_length=256, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = tokenized['input_ids'].squeeze().tolist()\n",
    "    attention_mask = tokenized['attention_mask'].squeeze().tolist()\n",
    "    labels = input_ids[1:] + [tokenizer.pad_token_id]\n",
    "    data.append({\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\":attention_mask})\n",
    "\n",
    "\n",
    "input_ids = [item[\"input_ids\"] for item in data]\n",
    "labels = [item[\"labels\"] for item in data]\n",
    "attention_mask = [item[\"attention_mask\"] for item in data]\n",
    "\n",
    "input_ids = batch(input_ids)\n",
    "labels = batch(labels)\n",
    "attention_mask = batch(attention_mask)\n",
    "\n",
    "input_ids_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "attention_mask_tensor = torch.tensor(attention_mask, dtype=torch.long)\n",
    "\n",
    "\n",
    "# 仮定: ボキャブラリサイズと頻出語のトークンIDを定義\n",
    "vocab_size = model.config.vocab_size\n",
    "\n",
    "# クロスエントロピー損失関数の設定\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=128001)\n",
    "criterion.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "input_ids_tensor=input_ids_tensor.to(device)\n",
    "labels_tensor=labels_tensor.to(device)\n",
    "attention_mask_tensor = attention_mask_tensor.to(device)\n",
    "model.to(device)\n",
    "teacher_model.to(device)\n",
    "model.eval()\n",
    "criterion.to(device)\n",
    "alpha=0.5\n",
    "temperature=1.0\n",
    "epochs = 1\n",
    "\n",
    "i=0\n",
    "\n",
    "input_ids=input_ids_tensor[i]\n",
    "labels=labels_tensor[i]\n",
    "attention_mask=attention_mask_tensor[i]\n",
    "optimizer.zero_grad()\n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "logits=outputs.logits\n",
    "with torch.no_grad():\n",
    "    student_prob=F.log_softmax(logits, dim=-1)\n",
    "    teacher_outputs_logits=teacher_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).logits\n",
    "    teacher_prob=F.softmax(teacher_outputs_logits, dim=-1)\n",
    "\n",
    "kldiv_loss=F.kl_div(student_prob, teacher_prob, reduction=\"none\")\n",
    "kl_div_per_token = kldiv_loss.sum(dim=-1)\n",
    "kl_loss=(kl_div_per_token * attention_mask).sum()/attention_mask.sum()\n",
    "\n",
    "print(kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 128256])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kldiv_loss.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_div_per_token.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.view(-1).size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\")\n",
    "device='cuda'\n",
    "# モデルの準備\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "data_size = 100\n",
    "size = int(data_size/4)\n",
    "\n",
    "train_dataset=ds[\"train\"].select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ''}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' = Valkyria Chronicles III = \\n'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ''}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|> Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3, lit. Valkyria of the Battlefield 3 ), commonly referred to as Valkyria Chronicles III outside Japan, is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable. Released in January 2011 in Japan, it is the third game in the Valkyria series. Employing the same fusion of tactical and real @-@ time gameplay as its predecessors, the story runs parallel to the first game and follows the \" Nameless \", a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \". \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(train_dataset[3][\"text\"])['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.4231e-05, device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kldiv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128256) must match the size of tensor b (256) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mkldiv_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128256) must match the size of tensor b (256) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "kldiv_loss * attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_div_per_token = kldiv_loss.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.4868,  3.5298,  8.3883,  ..., 12.7773, 12.8699, 13.3185],\n",
       "        [ 7.4868,  5.3422,  5.7633,  ..., 12.2131, 12.1325, 12.3083],\n",
       "        [ 7.4868,  9.4952,  6.7543,  ..., 11.2674, 11.4796, 12.0311],\n",
       "        [ 7.4868, 10.5451,  8.2415,  ..., 11.1294, 11.3222, 11.2411]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_div_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.6776, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kl_div_per_token * attention_mask).sum()/attention_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23.5073, device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_div_per_token.sum()/attention_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\")\n",
    "device='cuda'\n",
    "# モデルの準備\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./model/initialized_distill_model2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128001\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums=[1,2,3,5,5,[0]*10]*10\n",
    "with open(\"output.txt\", \"w\") as f:\n",
    "            f.write(str(sums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
