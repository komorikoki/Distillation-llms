{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0') {'input_ids': tensor([[128000,  10149,   5708,   7729,  10205,    304,   6841,     30]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,  10149,   5708,   7729,  10205,    304,   6841,     30,   6841,\n",
      "            374,    264,   1401,   2305,    369,    279,   2326,   5708,   3157,\n",
      "             11,    323,   1690,  15167,    527,  23132,    264,   3831,   5178,\n",
      "            304,    279,   2132,   4376,    315,    279,   1060,     13,   4452,\n",
      "             11,    433,    596,    539,    264,  15803,    430,   5708,   7729,\n",
      "            690,  10205,    304,   6841,    382,   8538,   3284,   8125,    369,\n",
      "            264,  18174,    304,   5708,   7729,    304,   6841,   2997,   1473,\n",
      "              9,    256,   3146,  39922,  53838,  96618,    578,   5708,   3157,\n",
      "            649,    387,  17509,     11,    323,   6841,   1253,   3217,    264,\n",
      "          76506,   4245,    311,   5370,   3157,   9547,     11,   1778,    439,\n",
      "           7100,  34824,     11,   2802,   7969,     11,    477,  87998,   4455,\n",
      "            627]], device='cuda:0')\n",
      "Will stock prices rise in November? November is a key month for the US stock market, and many investors are expecting a strong performance in the second half of the year. However, it's not a guarantee that stock prices will rise in November.\n",
      "\n",
      "Some possible reasons for a decline in stock prices in November include:\n",
      "\n",
      "*   **Market volatility**: The stock market can be volatile, and November may experience a downturn due to various market factors, such as economic indicators, interest rates, or geopolitical events.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 質問の入力\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "device='cuda'\n",
    "question = \"Will stock prices rise in November?\"\n",
    "input_ids = tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "model.to(device)\n",
    "\n",
    "attention_mask=input_ids['attention_mask']\n",
    "print(attention_mask, input_ids)\n",
    "\n",
    "output = model.generate(input_ids['input_ids'], attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id, max_length=100)\n",
    "print(output)\n",
    "\n",
    "# 出力のデコード\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128256])\n"
     ]
    }
   ],
   "source": [
    "question = \"b b\"\n",
    "input_ids = tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "output = model(input_ids['input_ids'], attention_mask=attention_mask)\n",
    "print(output.logits.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.7209e-04, 8.4792e-05, 1.1225e-04,  ..., 3.2487e-08, 3.2505e-08,\n",
      "        3.2504e-08], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor(293, device='cuda:0')\n",
      " b\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "probabilities = torch.softmax(output.logits[0][2], dim=-1)\n",
    "print(probabilities)\n",
    "predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "print(predicted_token_ids)\n",
    "decoded_text = tokenizer.decode(predicted_token_ids, skip_special_tokens=True)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": [\n",
       "    128001,\n",
       "    128008,\n",
       "    128009\n",
       "  ],\n",
       "  \"temperature\": 0.6,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForCausalLM                                        --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   262,668,288\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-5                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-6                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-7                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-8                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-9                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-10                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-11                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-12                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-13                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-14                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-15                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-16                     60,821,504\n",
       "│    └─LlamaRMSNorm: 2-3                                2,048\n",
       "│    └─LlamaRotaryEmbedding: 2-4                        --\n",
       "├─Linear: 1-2                                           262,668,288\n",
       "================================================================================\n",
       "Total params: 1,498,482,688\n",
       "Trainable params: 1,498,482,688\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distillLLAMA\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distillLLAMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForCausalLM                                        --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   262,668,288\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-5                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-6                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-7                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-8                      60,821,504\n",
       "│    └─LlamaRMSNorm: 2-3                                2,048\n",
       "│    └─LlamaRotaryEmbedding: 2-4                        --\n",
       "├─Linear: 1-2                                           262,668,288\n",
       "================================================================================\n",
       "Total params: 1,011,910,656\n",
       "Trainable params: 1,011,910,656\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/my-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForCausalLM                                        --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   262,668,288\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      60,821,504\n",
       "│    └─LlamaRMSNorm: 2-3                                2,048\n",
       "│    └─LlamaRotaryEmbedding: 2-4                        --\n",
       "├─Linear: 1-2                                           262,668,288\n",
       "================================================================================\n",
       "Total params: 768,624,640\n",
       "Trainable params: 768,624,640\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distillLLAMA2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distillLLAMA2\")\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(model, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
