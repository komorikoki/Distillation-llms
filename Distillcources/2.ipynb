{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForCausalLM                                        --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   262,668,288\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-1                --\n",
       "│    │    │    │    └─Linear: 5-1                       4,194,304\n",
       "│    │    │    │    └─Linear: 5-2                       1,048,576\n",
       "│    │    │    │    └─Linear: 5-3                       1,048,576\n",
       "│    │    │    │    └─Linear: 5-4                       4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-5         --\n",
       "│    │    │    └─LlamaMLP: 4-2                          --\n",
       "│    │    │    │    └─Linear: 5-6                       16,777,216\n",
       "│    │    │    │    └─Linear: 5-7                       16,777,216\n",
       "│    │    │    │    └─Linear: 5-8                       16,777,216\n",
       "│    │    │    │    └─SiLU: 5-9                         --\n",
       "│    │    │    └─LlamaRMSNorm: 4-3                      2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-4                      2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-5                --\n",
       "│    │    │    │    └─Linear: 5-10                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-11                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-12                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-13                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-14        --\n",
       "│    │    │    └─LlamaMLP: 4-6                          --\n",
       "│    │    │    │    └─Linear: 5-15                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-16                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-17                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-18                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-7                      2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-8                      2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-9                --\n",
       "│    │    │    │    └─Linear: 5-19                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-20                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-21                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-22                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-23        --\n",
       "│    │    │    └─LlamaMLP: 4-10                         --\n",
       "│    │    │    │    └─Linear: 5-24                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-25                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-26                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-27                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-11                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-12                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-13               --\n",
       "│    │    │    │    └─Linear: 5-28                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-29                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-30                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-31                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-32        --\n",
       "│    │    │    └─LlamaMLP: 4-14                         --\n",
       "│    │    │    │    └─Linear: 5-33                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-34                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-35                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-36                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-15                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-16                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-5                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-17               --\n",
       "│    │    │    │    └─Linear: 5-37                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-38                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-39                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-40                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-41        --\n",
       "│    │    │    └─LlamaMLP: 4-18                         --\n",
       "│    │    │    │    └─Linear: 5-42                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-43                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-44                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-45                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-19                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-20                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-6                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-21               --\n",
       "│    │    │    │    └─Linear: 5-46                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-47                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-48                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-49                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-50        --\n",
       "│    │    │    └─LlamaMLP: 4-22                         --\n",
       "│    │    │    │    └─Linear: 5-51                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-52                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-53                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-54                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-23                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-24                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-7                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-25               --\n",
       "│    │    │    │    └─Linear: 5-55                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-56                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-57                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-58                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-59        --\n",
       "│    │    │    └─LlamaMLP: 4-26                         --\n",
       "│    │    │    │    └─Linear: 5-60                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-61                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-62                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-63                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-27                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-28                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-8                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-29               --\n",
       "│    │    │    │    └─Linear: 5-64                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-65                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-66                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-67                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-68        --\n",
       "│    │    │    └─LlamaMLP: 4-30                         --\n",
       "│    │    │    │    └─Linear: 5-69                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-70                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-71                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-72                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-31                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-32                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-9                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-33               --\n",
       "│    │    │    │    └─Linear: 5-73                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-74                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-75                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-76                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-77        --\n",
       "│    │    │    └─LlamaMLP: 4-34                         --\n",
       "│    │    │    │    └─Linear: 5-78                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-79                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-80                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-81                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-35                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-36                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-10                     --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-37               --\n",
       "│    │    │    │    └─Linear: 5-82                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-83                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-84                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-85                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-86        --\n",
       "│    │    │    └─LlamaMLP: 4-38                         --\n",
       "│    │    │    │    └─Linear: 5-87                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-88                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-89                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-90                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-39                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-40                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-11                     --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-41               --\n",
       "│    │    │    │    └─Linear: 5-91                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-92                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-93                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-94                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-95        --\n",
       "│    │    │    └─LlamaMLP: 4-42                         --\n",
       "│    │    │    │    └─Linear: 5-96                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-97                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-98                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-99                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-43                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-44                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-12                     --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-45               --\n",
       "│    │    │    │    └─Linear: 5-100                     4,194,304\n",
       "│    │    │    │    └─Linear: 5-101                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-102                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-103                     4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-104       --\n",
       "│    │    │    └─LlamaMLP: 4-46                         --\n",
       "│    │    │    │    └─Linear: 5-105                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-106                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-107                     16,777,216\n",
       "│    │    │    │    └─SiLU: 5-108                       --\n",
       "│    │    │    └─LlamaRMSNorm: 4-47                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-48                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-13                     --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-49               --\n",
       "│    │    │    │    └─Linear: 5-109                     4,194,304\n",
       "│    │    │    │    └─Linear: 5-110                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-111                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-112                     4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-113       --\n",
       "│    │    │    └─LlamaMLP: 4-50                         --\n",
       "│    │    │    │    └─Linear: 5-114                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-115                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-116                     16,777,216\n",
       "│    │    │    │    └─SiLU: 5-117                       --\n",
       "│    │    │    └─LlamaRMSNorm: 4-51                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-52                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-14                     --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-53               --\n",
       "│    │    │    │    └─Linear: 5-118                     4,194,304\n",
       "│    │    │    │    └─Linear: 5-119                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-120                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-121                     4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-122       --\n",
       "│    │    │    └─LlamaMLP: 4-54                         --\n",
       "│    │    │    │    └─Linear: 5-123                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-124                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-125                     16,777,216\n",
       "│    │    │    │    └─SiLU: 5-126                       --\n",
       "│    │    │    └─LlamaRMSNorm: 4-55                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-56                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-15                     --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-57               --\n",
       "│    │    │    │    └─Linear: 5-127                     4,194,304\n",
       "│    │    │    │    └─Linear: 5-128                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-129                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-130                     4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-131       --\n",
       "│    │    │    └─LlamaMLP: 4-58                         --\n",
       "│    │    │    │    └─Linear: 5-132                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-133                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-134                     16,777,216\n",
       "│    │    │    │    └─SiLU: 5-135                       --\n",
       "│    │    │    └─LlamaRMSNorm: 4-59                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-60                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-16                     --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-61               --\n",
       "│    │    │    │    └─Linear: 5-136                     4,194,304\n",
       "│    │    │    │    └─Linear: 5-137                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-138                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-139                     4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-140       --\n",
       "│    │    │    └─LlamaMLP: 4-62                         --\n",
       "│    │    │    │    └─Linear: 5-141                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-142                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-143                     16,777,216\n",
       "│    │    │    │    └─SiLU: 5-144                       --\n",
       "│    │    │    └─LlamaRMSNorm: 4-63                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-64                     2,048\n",
       "│    └─LlamaRMSNorm: 2-3                                2,048\n",
       "│    └─LlamaRotaryEmbedding: 2-4                        --\n",
       "├─Linear: 1-2                                           262,668,288\n",
       "================================================================================\n",
       "Total params: 1,498,482,688\n",
       "Trainable params: 1,498,482,688\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 質問の入力\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a [MASK] great.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   2028,    374,    264,    510,  50963,     60,   2294,     13]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device='cuda'\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "model.to(device)\n",
    "token_logits = model(**inputs).logits\n",
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASK\n",
      "50963\n"
     ]
    }
   ],
   "source": [
    "mask_token=tokenizer('MASK', return_tensors=\"pt\")\n",
    "mask_token=mask_token['input_ids']\n",
    "mask_token_id=mask_token[0]\n",
    "mask_token_id=mask_token_id[1].item()\n",
    "mask_token='MASK'\n",
    "print(mask_token)\n",
    "print(mask_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> This is a []] great.'\n",
      "'>>> This is a [].] great.'\n",
      "'>>> This is a []\n",
      "\n",
      "] great.'\n",
      "'>>> This is a []\n",
      "] great.'\n",
      "'>>> This is a [](] great.'\n"
     ]
    }
   ],
   "source": [
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    token=tokenizer.decode([token])\n",
    "    print(f\"'>>> {text.replace(mask_token, token)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> ]'\n",
      "'>>> ].'\n",
      "'>>> ]\n",
      "\n",
      "'\n",
      "'>>> ]\n",
      "'\n",
      "'>>> ]('\n"
     ]
    }
   ],
   "source": [
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {tokenizer.decode([token])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
