{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"model/normal_model_distill\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力文\n",
    "input_text = \"The weather today is\"\n",
    "\n",
    "# トークン化\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather today is unusual although they are\n"
     ]
    }
   ],
   "source": [
    "# 4単語生成（トークン数に応じて調整）\n",
    "output_ids = model.generate(input_ids, max_new_tokens=4, do_sample=True, top_k=50)\n",
    "\n",
    "# トークンを文章にデコード\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 結果を表示\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset: 100%|██████████| 10000/10000 [00:03<00:00, 2681.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "def reshape(dataset):\n",
    "    reshape_dataset = [0] * len(dataset)\n",
    "    for i in range(len(dataset)):\n",
    "        reshape_dataset[i]=\"C: \"+dataset[i][\"context\"]+\" Q: \"+dataset[i][\"question\"]+\" A: \"+dataset[i][\"answers\"][\"text\"][0]\n",
    "    reshape_dataset = [item for item in reshape_dataset if item != '' and len(item) >= 50 and '@' not in item]\n",
    "    reshape_dataset = [re.sub(r'[^a-zA-Z0-9 .:?]', '', item) for item in reshape_dataset]\n",
    "    reshape_dataset = [re.sub(r'\\s+', ' ', item) for item in reshape_dataset]\n",
    "    return reshape_dataset[:data_size]\n",
    "\n",
    "def max_length(dataset):\n",
    "    max_eval=0\n",
    "    for i in dataset:\n",
    "        max_eval = len(i) if len(i) > max_eval else max_eval\n",
    "    print(max_eval)\n",
    "    return\n",
    "\n",
    "def batch(input, size):\n",
    "    batch_train=[]\n",
    "    for i in range(size):\n",
    "        batch_input=[input[4*i+0], input[4*i+1], input[4*i+2], input[4*i+3]]\n",
    "        batch_train.append(batch_input)\n",
    "\n",
    "    return batch_train\n",
    "\n",
    "def make_data(data):\n",
    "    maxlen=0\n",
    "    dataset=reshape(data)\n",
    "    data = []\n",
    "    for text in tqdm(dataset, desc=\"Tokenizing dataset\"):\n",
    "        tokenized = len(tokenizer(text)['input_ids'])\n",
    "        maxlen += tokenized\n",
    "    \n",
    "    return maxlen/10000\n",
    "\n",
    "def make_tensor(data, type, size):\n",
    "    tmp = [item[type] for item in data]\n",
    "    tmp = batch(tmp, size)\n",
    "    tensor=torch.tensor(tmp, dtype=torch.long)\n",
    "    return tensor\n",
    "\n",
    "ds = load_dataset(\"rajpurkar/squad\")\n",
    "device='cuda'\n",
    "student_model = AutoModelForCausalLM.from_pretrained(\"../model/distill_model\")\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\"./model/teacher_model_finetuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "data_size = 10000\n",
    "\n",
    "size = int(data_size/4)\n",
    "\n",
    "train_dataset=ds[\"train\"].shuffle(seed=42)\n",
    "\n",
    "data = make_data(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170.2472"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
