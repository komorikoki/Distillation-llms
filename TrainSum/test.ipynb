{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\")\n",
    "device='cuda'\n",
    "# モデルの準備\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "student_model = AutoModelForCausalLM.from_pretrained(\"./model/initialized_distill_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128001\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForCausalLM                                        --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   262,668,288\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-5                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-6                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-7                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-8                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-9                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-10                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-11                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-12                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-13                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-14                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-15                     60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-16                     60,821,504\n",
       "│    └─LlamaRMSNorm: 2-3                                2,048\n",
       "│    └─LlamaRotaryEmbedding: 2-4                        --\n",
       "├─Linear: 1-2                                           262,668,288\n",
       "================================================================================\n",
       "Total params: 1,498,482,688\n",
       "Trainable params: 1,498,482,688\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(teacher_model, depth=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForCausalLM                                        --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   262,668,288\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-5                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-6                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-7                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-8                      60,821,504\n",
       "│    └─LlamaRMSNorm: 2-3                                2,048\n",
       "│    └─LlamaRotaryEmbedding: 2-4                        --\n",
       "├─Linear: 1-2                                           262,668,288\n",
       "================================================================================\n",
       "Total params: 1,011,910,656\n",
       "Trainable params: 1,011,910,656\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(student_model, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101021\n",
      "4130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset: 100%|██████████| 100000/100000 [00:38<00:00, 2600.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.6776, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\")\n",
    "device='cuda'\n",
    "# モデルの準備\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./model/initialized_distill_model2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "data_size = 100000\n",
    "size = int(data_size/4)\n",
    "\n",
    "train_dataset=ds[\"train\"].shuffle(seed=42).select(range(500000))\n",
    "\n",
    "def reshape(dataset):\n",
    "    dataset=dataset[\"text\"]\n",
    "    dataset = [item for item in dataset if item != '' and len(item) >= 50 and '@' not in item]\n",
    "    dataset = [re.sub(r'[^a-zA-Z0-9 ?]', '', item) for item in dataset]\n",
    "    dataset = [re.sub(r'\\s+', ' ', item) for item in dataset]\n",
    "    print(len(dataset))\n",
    "    return dataset[:data_size]\n",
    "\n",
    "def max_length(dataset):\n",
    "    max_eval=0\n",
    "    for i in dataset:\n",
    "        max_eval = len(i) if len(i) > max_eval else max_eval\n",
    "    print(max_eval)\n",
    "    return\n",
    "\n",
    "\n",
    "dataset=reshape(train_dataset)\n",
    "max_length(dataset)\n",
    "\n",
    "def batch(input):\n",
    "    batch_train=[]\n",
    "    for i in range(size):\n",
    "        batch_input=[input[4*i+0], input[4*i+1], input[4*i+2], input[4*i+3]]\n",
    "        batch_train.append(batch_input)\n",
    "\n",
    "    return batch_train\n",
    "\n",
    "# 入力とラベルを設定\n",
    "data = []\n",
    "for text in tqdm(dataset, desc=\"Tokenizing dataset\"):\n",
    "    tokenized = tokenizer(text, padding=\"max_length\", max_length=256, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = tokenized['input_ids'].squeeze().tolist()\n",
    "    attention_mask = tokenized['attention_mask'].squeeze().tolist()\n",
    "    labels = input_ids[1:] + [tokenizer.pad_token_id]\n",
    "    data.append({\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\":attention_mask})\n",
    "\n",
    "\n",
    "input_ids = [item[\"input_ids\"] for item in data]\n",
    "labels = [item[\"labels\"] for item in data]\n",
    "attention_mask = [item[\"attention_mask\"] for item in data]\n",
    "\n",
    "input_ids = batch(input_ids)\n",
    "labels = batch(labels)\n",
    "attention_mask = batch(attention_mask)\n",
    "\n",
    "input_ids_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "attention_mask_tensor = torch.tensor(attention_mask, dtype=torch.long)\n",
    "\n",
    "\n",
    "# 仮定: ボキャブラリサイズと頻出語のトークンIDを定義\n",
    "vocab_size = model.config.vocab_size\n",
    "\n",
    "# クロスエントロピー損失関数の設定\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=128001)\n",
    "criterion.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "input_ids_tensor=input_ids_tensor.to(device)\n",
    "labels_tensor=labels_tensor.to(device)\n",
    "attention_mask_tensor = attention_mask_tensor.to(device)\n",
    "model.to(device)\n",
    "teacher_model.to(device)\n",
    "model.eval()\n",
    "criterion.to(device)\n",
    "alpha=0.5\n",
    "temperature=1.0\n",
    "epochs = 1\n",
    "\n",
    "i=0\n",
    "\n",
    "input_ids=input_ids_tensor[i]\n",
    "labels=labels_tensor[i]\n",
    "attention_mask=attention_mask_tensor[i]\n",
    "optimizer.zero_grad()\n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "logits=outputs.logits\n",
    "with torch.no_grad():\n",
    "    student_prob=F.log_softmax(logits, dim=-1)\n",
    "    teacher_outputs_logits=teacher_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).logits\n",
    "    teacher_prob=F.softmax(teacher_outputs_logits, dim=-1)\n",
    "\n",
    "kldiv_loss=F.kl_div(student_prob, teacher_prob, reduction=\"none\")\n",
    "kl_div_per_token = kldiv_loss.sum(dim=-1)\n",
    "kl_loss=(kl_div_per_token * attention_mask).sum()/attention_mask.sum()\n",
    "\n",
    "print(kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.7219e-04,  6.4446e-03,  6.6242e-01,  ..., -1.9534e-08,\n",
       "          -1.2769e-08, -2.4000e-08],\n",
       "         [ 7.3123e-08,  2.4769e-05, -9.1078e-07,  ..., -8.1722e-09,\n",
       "          -7.8389e-09, -9.0286e-09],\n",
       "         [-4.6580e-06,  2.8261e-06, -1.5009e-08,  ..., -1.0039e-08,\n",
       "          -9.0169e-09, -8.6702e-09],\n",
       "         ...,\n",
       "         [-4.3328e-08, -8.8014e-07, -3.7146e-07,  ..., -9.1351e-08,\n",
       "          -6.3438e-08, -7.6447e-08],\n",
       "         [-9.7097e-07, -9.7997e-07, -3.7900e-07,  ..., -8.9757e-08,\n",
       "          -6.0741e-08, -7.0755e-08],\n",
       "         [-1.0289e-06, -1.1872e-06, -4.4356e-07,  ..., -9.1538e-08,\n",
       "          -6.3831e-08, -7.3005e-08]],\n",
       "\n",
       "        [[ 4.7219e-04,  6.4446e-03,  6.6242e-01,  ..., -1.9534e-08,\n",
       "          -1.2769e-08, -2.4000e-08],\n",
       "         [ 1.0664e-04,  5.8025e-06,  8.3375e-06,  ..., -4.5780e-09,\n",
       "          -3.5949e-09, -6.2190e-09],\n",
       "         [ 4.4918e-04,  2.1684e-05,  5.4156e-06,  ..., -5.0706e-09,\n",
       "          -4.8161e-09, -5.9528e-09],\n",
       "         ...,\n",
       "         [-1.0863e-06, -1.0065e-07, -2.4456e-07,  ..., -1.0671e-08,\n",
       "          -8.7112e-09, -8.3283e-09],\n",
       "         [-1.0263e-06, -1.0006e-07, -2.4499e-07,  ..., -1.0000e-08,\n",
       "          -8.5876e-09, -7.8516e-09],\n",
       "         [-9.8646e-07, -9.6379e-08, -2.4386e-07,  ..., -9.9333e-09,\n",
       "          -8.8360e-09, -7.8675e-09]],\n",
       "\n",
       "        [[ 4.7219e-04,  6.4446e-03,  6.6242e-01,  ..., -1.9534e-08,\n",
       "          -1.2769e-08, -2.4000e-08],\n",
       "         [ 2.4608e-06,  3.3276e-06, -1.4551e-06,  ..., -3.6406e-09,\n",
       "          -2.6974e-09, -4.9596e-09],\n",
       "         [ 2.3609e-04,  3.2590e-05,  3.4730e-05,  ..., -3.4945e-09,\n",
       "          -3.1285e-09, -4.2423e-09],\n",
       "         ...,\n",
       "         [-1.0335e-06, -3.4107e-07, -4.8502e-07,  ..., -1.9496e-08,\n",
       "          -1.9313e-08, -2.3296e-08],\n",
       "         [-5.5091e-07, -3.5679e-07, -4.9817e-07,  ..., -2.0043e-08,\n",
       "          -1.9181e-08, -2.4053e-08],\n",
       "         [-3.3143e-07, -3.4832e-07, -5.6093e-07,  ..., -2.0314e-08,\n",
       "          -1.8496e-08, -2.4361e-08]],\n",
       "\n",
       "        [[ 4.7219e-04,  6.4446e-03,  6.6242e-01,  ..., -1.9534e-08,\n",
       "          -1.2769e-08, -2.4000e-08],\n",
       "         [ 2.1896e-04,  2.5998e-06,  1.4154e-06,  ..., -3.9823e-09,\n",
       "          -3.4129e-09, -4.4942e-09],\n",
       "         [ 8.2731e-08, -5.4671e-07, -6.1382e-07,  ..., -4.9346e-09,\n",
       "          -3.7524e-09, -5.5342e-09],\n",
       "         ...,\n",
       "         [-4.1078e-07, -1.1240e-07, -2.7948e-07,  ..., -1.1474e-08,\n",
       "          -1.3229e-08, -1.4628e-08],\n",
       "         [-5.7218e-07, -8.5288e-08, -2.3778e-07,  ..., -1.1234e-08,\n",
       "          -1.2337e-08, -1.4396e-08],\n",
       "         [-5.9960e-07, -9.3085e-08, -2.3573e-07,  ..., -1.2065e-08,\n",
       "          -1.3022e-08, -1.4463e-08]]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kldiv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.view(-1).size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutkldiv(kldiv_loss, labels):\n",
    "    klloss=0\n",
    "    for i in range(labels.size(0)):\n",
    "        if labels[i] != 128001:\n",
    "            kldiv_loss[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.4231e-05, device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kldiv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128256) must match the size of tensor b (256) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mkldiv_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128256) must match the size of tensor b (256) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "kldiv_loss * attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_div_per_token = kldiv_loss.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.4868,  3.5298,  8.3883,  ..., 12.7773, 12.8699, 13.3185],\n",
       "        [ 7.4868,  5.3422,  5.7633,  ..., 12.2131, 12.1325, 12.3083],\n",
       "        [ 7.4868,  9.4952,  6.7543,  ..., 11.2674, 11.4796, 12.0311],\n",
       "        [ 7.4868, 10.5451,  8.2415,  ..., 11.1294, 11.3222, 11.2411]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_div_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.6776, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kl_div_per_token * attention_mask).sum()/attention_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23.5073, device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_div_per_token.sum()/attention_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\")\n",
    "device='cuda'\n",
    "# モデルの準備\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./model/initialized_distill_model2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128001\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
