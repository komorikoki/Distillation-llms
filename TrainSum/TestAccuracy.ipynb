{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767\n",
      "2084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset: 100%|██████████| 766/766 [00:00<00:00, 3261.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\")\n",
    "device='cuda'\n",
    "# モデルの準備\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./finetuned_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "data_size = 766\n",
    "size = int(data_size/4)\n",
    "\n",
    "validation_dataset=ds[\"validation\"].shuffle(seed=42)\n",
    "\n",
    "def reshape(dataset):\n",
    "    dataset=dataset[\"text\"]\n",
    "    dataset = [item for item in dataset if item != '' and len(item) >= 50 and '@' not in item]\n",
    "    dataset = [re.sub(r'[^a-zA-Z0-9 ?]', '', item) for item in dataset]\n",
    "    dataset = [re.sub(r'\\s+', ' ', item) for item in dataset]\n",
    "    print(len(dataset))\n",
    "    return dataset[:data_size]\n",
    "\n",
    "def max_length(dataset):\n",
    "    max_eval=0\n",
    "    for i in dataset:\n",
    "        max_eval = len(i) if len(i) > max_eval else max_eval\n",
    "    print(max_eval)\n",
    "    return\n",
    "\n",
    "\n",
    "dataset=reshape(validation_dataset)\n",
    "max_length(dataset)\n",
    "\n",
    "def batch(input):\n",
    "    batch_train=[]\n",
    "    for i in range(size):\n",
    "        batch_input=[input[4*i+0], input[4*i+1], input[4*i+2], input[4*i+3]]\n",
    "        batch_train.append(batch_input)\n",
    "\n",
    "    return batch_train\n",
    "\n",
    "# 入力とラベルを設定\n",
    "data = []\n",
    "for text in tqdm(dataset, desc=\"Tokenizing dataset\"):\n",
    "    tokenized = tokenizer(text, padding=\"max_length\", max_length=256, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = tokenized['input_ids'].squeeze().tolist()\n",
    "    attention_mask = tokenized['attention_mask'].squeeze().tolist()\n",
    "    labels = input_ids[1:] + [tokenizer.pad_token_id]\n",
    "    data.append({\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\":attention_mask})\n",
    "\n",
    "\n",
    "input_ids = [item[\"input_ids\"] for item in data]\n",
    "labels = [item[\"labels\"] for item in data]\n",
    "attention_mask = [item[\"attention_mask\"] for item in data]\n",
    "\n",
    "input_ids = batch(input_ids)\n",
    "labels = batch(labels)\n",
    "attention_mask = batch(attention_mask)\n",
    "\n",
    "input_ids_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "attention_mask_tensor = torch.tensor(attention_mask, dtype=torch.long)\n",
    "\n",
    "\n",
    "# 仮定: ボキャブラリサイズと頻出語のトークンIDを定義\n",
    "vocab_size = model.config.vocab_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128009"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=128009)\n",
    "criterion.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "input_ids_tensor=input_ids_tensor.to(device)\n",
    "labels_tensor=labels_tensor.to(device)\n",
    "attention_mask_tensor = attention_mask_tensor.to(device)\n",
    "\n",
    "model.to(device)\n",
    "eval_loss=0\n",
    "model.eval()\n",
    "alpha=0.5\n",
    "temperature=1.0\n",
    "\n",
    "i=0\n",
    "    \n",
    "input_ids=input_ids_tensor[i]\n",
    "labels=labels_tensor[i]\n",
    "attention_mask=attention_mask_tensor[i]\n",
    "optimizer.zero_grad()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    logits = outputs.logits\n",
    "\n",
    "loss = criterion(logits.view(-1, vocab_size), labels.view(-1))\n",
    "\n",
    "# 上位3つを取得\n",
    "top_k_values, top_k_indices = torch.topk(logits, k=5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9553, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
