{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torchinfo import summary\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "distillmodel0 = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "distillmodel1 = AutoModelForCausalLM.from_pretrained(\"distillLLAMA\")\n",
    "distillmodel2 = AutoModelForCausalLM.from_pretrained(\"distillLLAMA2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForCausalLM                                        --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   262,668,288\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      60,821,504\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      60,821,504\n",
       "│    └─LlamaRMSNorm: 2-3                                2,048\n",
       "│    └─LlamaRotaryEmbedding: 2-4                        --\n",
       "├─Linear: 1-2                                           262,668,288\n",
       "================================================================================\n",
       "Total params: 768,624,640\n",
       "Trainable params: 768,624,640\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(distillmodel0, depth=3)\n",
    "summary(distillmodel1, depth=3)\n",
    "summary(distillmodel2, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the color of apple? red.\n",
      "Is it true that apple is a fruit that is\n",
      "what is the color of apple?arsearsearsearsearsearsearsearsearsearsearsearse\n",
      "what is the color of apple? inject3333333,{{{\n"
     ]
    }
   ],
   "source": [
    "# 質問の入力\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "device='cuda'\n",
    "question = \"what is the color of apple ?\"\n",
    "input_ids = tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "\n",
    "attention_mask=input_ids['attention_mask']\n",
    "\n",
    "distillmodel0.to(device)\n",
    "output = distillmodel0.generate(input_ids['input_ids'], attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id, max_length=20)\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(answer)\n",
    "\n",
    "distillmodel1.to(device)\n",
    "output = distillmodel1.generate(input_ids['input_ids'], attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id, max_length=20)\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(answer)\n",
    "\n",
    "distillmodel2.to(device)\n",
    "output = distillmodel2.generate(input_ids['input_ids'], attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id, max_length=20)\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the color of apple?arsearsearsearsearsearsearsearsearsearsearsearsearsearsearsearsearsearsearsearsearsearsearsearsearsearsearse воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду воду\n"
     ]
    }
   ],
   "source": [
    "# 質問の入力\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "device='cuda'\n",
    "question = \"what is the color of apple ?\"\n",
    "input_ids = tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "distillmodel1.to(device)\n",
    "\n",
    "attention_mask=input_ids['attention_mask']\n",
    "\n",
    "output0 = distillmodel0.generate(input_ids['input_ids'], attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id, max_length=100)\n",
    "output1 = distillmodel1.generate(input_ids['input_ids'], attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id, max_length=100)\n",
    "output2 = distillmodel2.generate(input_ids['input_ids'], attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id, max_length=100)\n",
    "# 出力のデコード\n",
    "answer = tokenizer.decode(output0, skip_special_tokens=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "distillmodel2 = AutoModelForCausalLM.from_pretrained(\"distillLLAMA2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
