{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = AutoModelForCausalLM.from_pretrained(\"./model/teacher_model\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2541f320c39346da822fd1e9326898b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2 = AutoModelForCausalLM.from_pretrained(\"gpt2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForCausalLM                                        --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   262,668,288\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-1                --\n",
       "│    │    │    │    └─Linear: 5-1                       4,194,304\n",
       "│    │    │    │    └─Linear: 5-2                       1,048,576\n",
       "│    │    │    │    └─Linear: 5-3                       1,048,576\n",
       "│    │    │    │    └─Linear: 5-4                       4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-5         --\n",
       "│    │    │    └─LlamaMLP: 4-2                          --\n",
       "│    │    │    │    └─Linear: 5-6                       16,777,216\n",
       "│    │    │    │    └─Linear: 5-7                       16,777,216\n",
       "│    │    │    │    └─Linear: 5-8                       16,777,216\n",
       "│    │    │    │    └─SiLU: 5-9                         --\n",
       "│    │    │    └─LlamaRMSNorm: 4-3                      2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-4                      2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-5                --\n",
       "│    │    │    │    └─Linear: 5-10                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-11                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-12                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-13                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-14        --\n",
       "│    │    │    └─LlamaMLP: 4-6                          --\n",
       "│    │    │    │    └─Linear: 5-15                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-16                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-17                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-18                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-7                      2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-8                      2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-9                --\n",
       "│    │    │    │    └─Linear: 5-19                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-20                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-21                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-22                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-23        --\n",
       "│    │    │    └─LlamaMLP: 4-10                         --\n",
       "│    │    │    │    └─Linear: 5-24                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-25                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-26                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-27                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-11                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-12                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-13               --\n",
       "│    │    │    │    └─Linear: 5-28                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-29                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-30                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-31                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-32        --\n",
       "│    │    │    └─LlamaMLP: 4-14                         --\n",
       "│    │    │    │    └─Linear: 5-33                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-34                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-35                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-36                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-15                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-16                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-5                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-17               --\n",
       "│    │    │    │    └─Linear: 5-37                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-38                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-39                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-40                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-41        --\n",
       "│    │    │    └─LlamaMLP: 4-18                         --\n",
       "│    │    │    │    └─Linear: 5-42                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-43                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-44                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-45                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-19                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-20                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-6                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-21               --\n",
       "│    │    │    │    └─Linear: 5-46                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-47                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-48                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-49                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-50        --\n",
       "│    │    │    └─LlamaMLP: 4-22                         --\n",
       "│    │    │    │    └─Linear: 5-51                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-52                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-53                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-54                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-23                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-24                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-7                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-25               --\n",
       "│    │    │    │    └─Linear: 5-55                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-56                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-57                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-58                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-59        --\n",
       "│    │    │    └─LlamaMLP: 4-26                         --\n",
       "│    │    │    │    └─Linear: 5-60                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-61                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-62                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-63                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-27                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-28                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-8                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-29               --\n",
       "│    │    │    │    └─Linear: 5-64                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-65                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-66                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-67                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-68        --\n",
       "│    │    │    └─LlamaMLP: 4-30                         --\n",
       "│    │    │    │    └─Linear: 5-69                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-70                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-71                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-72                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-31                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-32                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-9                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-33               --\n",
       "│    │    │    │    └─Linear: 5-73                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-74                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-75                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-76                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-77        --\n",
       "│    │    │    └─LlamaMLP: 4-34                         --\n",
       "│    │    │    │    └─Linear: 5-78                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-79                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-80                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-81                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-35                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-36                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-10                     --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-37               --\n",
       "│    │    │    │    └─Linear: 5-82                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-83                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-84                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-85                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-86        --\n",
       "│    │    │    └─LlamaMLP: 4-38                         --\n",
       "│    │    │    │    └─Linear: 5-87                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-88                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-89                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-90                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-39                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-40                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-11                     --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-41               --\n",
       "│    │    │    │    └─Linear: 5-91                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-92                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-93                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-94                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-95        --\n",
       "│    │    │    └─LlamaMLP: 4-42                         --\n",
       "│    │    │    │    └─Linear: 5-96                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-97                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-98                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-99                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-43                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-44                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-12                     --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-45               --\n",
       "│    │    │    │    └─Linear: 5-100                     4,194,304\n",
       "│    │    │    │    └─Linear: 5-101                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-102                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-103                     4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-104       --\n",
       "│    │    │    └─LlamaMLP: 4-46                         --\n",
       "│    │    │    │    └─Linear: 5-105                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-106                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-107                     16,777,216\n",
       "│    │    │    │    └─SiLU: 5-108                       --\n",
       "│    │    │    └─LlamaRMSNorm: 4-47                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-48                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-13                     --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-49               --\n",
       "│    │    │    │    └─Linear: 5-109                     4,194,304\n",
       "│    │    │    │    └─Linear: 5-110                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-111                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-112                     4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-113       --\n",
       "│    │    │    └─LlamaMLP: 4-50                         --\n",
       "│    │    │    │    └─Linear: 5-114                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-115                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-116                     16,777,216\n",
       "│    │    │    │    └─SiLU: 5-117                       --\n",
       "│    │    │    └─LlamaRMSNorm: 4-51                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-52                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-14                     --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-53               --\n",
       "│    │    │    │    └─Linear: 5-118                     4,194,304\n",
       "│    │    │    │    └─Linear: 5-119                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-120                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-121                     4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-122       --\n",
       "│    │    │    └─LlamaMLP: 4-54                         --\n",
       "│    │    │    │    └─Linear: 5-123                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-124                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-125                     16,777,216\n",
       "│    │    │    │    └─SiLU: 5-126                       --\n",
       "│    │    │    └─LlamaRMSNorm: 4-55                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-56                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-15                     --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-57               --\n",
       "│    │    │    │    └─Linear: 5-127                     4,194,304\n",
       "│    │    │    │    └─Linear: 5-128                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-129                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-130                     4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-131       --\n",
       "│    │    │    └─LlamaMLP: 4-58                         --\n",
       "│    │    │    │    └─Linear: 5-132                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-133                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-134                     16,777,216\n",
       "│    │    │    │    └─SiLU: 5-135                       --\n",
       "│    │    │    └─LlamaRMSNorm: 4-59                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-60                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-16                     --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-61               --\n",
       "│    │    │    │    └─Linear: 5-136                     4,194,304\n",
       "│    │    │    │    └─Linear: 5-137                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-138                     1,048,576\n",
       "│    │    │    │    └─Linear: 5-139                     4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-140       --\n",
       "│    │    │    └─LlamaMLP: 4-62                         --\n",
       "│    │    │    │    └─Linear: 5-141                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-142                     16,777,216\n",
       "│    │    │    │    └─Linear: 5-143                     16,777,216\n",
       "│    │    │    │    └─SiLU: 5-144                       --\n",
       "│    │    │    └─LlamaRMSNorm: 4-63                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-64                     2,048\n",
       "│    └─LlamaRMSNorm: 2-3                                2,048\n",
       "│    └─LlamaRotaryEmbedding: 2-4                        --\n",
       "├─Linear: 1-2                                           262,668,288\n",
       "================================================================================\n",
       "Total params: 1,498,482,688\n",
       "Trainable params: 1,498,482,688\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model1, depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "GPT2LMHeadModel                                    --\n",
       "├─GPT2Model: 1-1                                   --\n",
       "│    └─Embedding: 2-1                              38,597,376\n",
       "│    └─Embedding: 2-2                              786,432\n",
       "│    └─Dropout: 2-3                                --\n",
       "│    └─ModuleList: 2-4                             --\n",
       "│    │    └─GPT2Block: 3-1                         --\n",
       "│    │    │    └─LayerNorm: 4-1                    1,536\n",
       "│    │    │    └─GPT2SdpaAttention: 4-2            2,362,368\n",
       "│    │    │    └─LayerNorm: 4-3                    1,536\n",
       "│    │    │    └─GPT2MLP: 4-4                      4,722,432\n",
       "│    │    └─GPT2Block: 3-2                         --\n",
       "│    │    │    └─LayerNorm: 4-5                    1,536\n",
       "│    │    │    └─GPT2SdpaAttention: 4-6            2,362,368\n",
       "│    │    │    └─LayerNorm: 4-7                    1,536\n",
       "│    │    │    └─GPT2MLP: 4-8                      4,722,432\n",
       "│    │    └─GPT2Block: 3-3                         --\n",
       "│    │    │    └─LayerNorm: 4-9                    1,536\n",
       "│    │    │    └─GPT2SdpaAttention: 4-10           2,362,368\n",
       "│    │    │    └─LayerNorm: 4-11                   1,536\n",
       "│    │    │    └─GPT2MLP: 4-12                     4,722,432\n",
       "│    │    └─GPT2Block: 3-4                         --\n",
       "│    │    │    └─LayerNorm: 4-13                   1,536\n",
       "│    │    │    └─GPT2SdpaAttention: 4-14           2,362,368\n",
       "│    │    │    └─LayerNorm: 4-15                   1,536\n",
       "│    │    │    └─GPT2MLP: 4-16                     4,722,432\n",
       "│    │    └─GPT2Block: 3-5                         --\n",
       "│    │    │    └─LayerNorm: 4-17                   1,536\n",
       "│    │    │    └─GPT2SdpaAttention: 4-18           2,362,368\n",
       "│    │    │    └─LayerNorm: 4-19                   1,536\n",
       "│    │    │    └─GPT2MLP: 4-20                     4,722,432\n",
       "│    │    └─GPT2Block: 3-6                         --\n",
       "│    │    │    └─LayerNorm: 4-21                   1,536\n",
       "│    │    │    └─GPT2SdpaAttention: 4-22           2,362,368\n",
       "│    │    │    └─LayerNorm: 4-23                   1,536\n",
       "│    │    │    └─GPT2MLP: 4-24                     4,722,432\n",
       "│    │    └─GPT2Block: 3-7                         --\n",
       "│    │    │    └─LayerNorm: 4-25                   1,536\n",
       "│    │    │    └─GPT2SdpaAttention: 4-26           2,362,368\n",
       "│    │    │    └─LayerNorm: 4-27                   1,536\n",
       "│    │    │    └─GPT2MLP: 4-28                     4,722,432\n",
       "│    │    └─GPT2Block: 3-8                         --\n",
       "│    │    │    └─LayerNorm: 4-29                   1,536\n",
       "│    │    │    └─GPT2SdpaAttention: 4-30           2,362,368\n",
       "│    │    │    └─LayerNorm: 4-31                   1,536\n",
       "│    │    │    └─GPT2MLP: 4-32                     4,722,432\n",
       "│    │    └─GPT2Block: 3-9                         --\n",
       "│    │    │    └─LayerNorm: 4-33                   1,536\n",
       "│    │    │    └─GPT2SdpaAttention: 4-34           2,362,368\n",
       "│    │    │    └─LayerNorm: 4-35                   1,536\n",
       "│    │    │    └─GPT2MLP: 4-36                     4,722,432\n",
       "│    │    └─GPT2Block: 3-10                        --\n",
       "│    │    │    └─LayerNorm: 4-37                   1,536\n",
       "│    │    │    └─GPT2SdpaAttention: 4-38           2,362,368\n",
       "│    │    │    └─LayerNorm: 4-39                   1,536\n",
       "│    │    │    └─GPT2MLP: 4-40                     4,722,432\n",
       "│    │    └─GPT2Block: 3-11                        --\n",
       "│    │    │    └─LayerNorm: 4-41                   1,536\n",
       "│    │    │    └─GPT2SdpaAttention: 4-42           2,362,368\n",
       "│    │    │    └─LayerNorm: 4-43                   1,536\n",
       "│    │    │    └─GPT2MLP: 4-44                     4,722,432\n",
       "│    │    └─GPT2Block: 3-12                        --\n",
       "│    │    │    └─LayerNorm: 4-45                   1,536\n",
       "│    │    │    └─GPT2SdpaAttention: 4-46           2,362,368\n",
       "│    │    │    └─LayerNorm: 4-47                   1,536\n",
       "│    │    │    └─GPT2MLP: 4-48                     4,722,432\n",
       "│    └─LayerNorm: 2-5                              1,536\n",
       "├─Linear: 1-2                                      38,597,376\n",
       "===========================================================================\n",
       "Total params: 163,037,184\n",
       "Trainable params: 163,037,184\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model2, depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight: requires_grad=True\n",
      "model.layers.0.self_attn.q_proj.weight: requires_grad=True\n",
      "model.layers.0.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.0.self_attn.v_proj.weight: requires_grad=True\n",
      "model.layers.0.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.0.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.0.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.0.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.0.input_layernorm.weight: requires_grad=True\n",
      "model.layers.0.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.1.self_attn.q_proj.weight: requires_grad=True\n",
      "model.layers.1.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.1.self_attn.v_proj.weight: requires_grad=True\n",
      "model.layers.1.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.1.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.1.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.1.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.1.input_layernorm.weight: requires_grad=True\n",
      "model.layers.1.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.2.self_attn.q_proj.weight: requires_grad=True\n",
      "model.layers.2.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.2.self_attn.v_proj.weight: requires_grad=True\n",
      "model.layers.2.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.2.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.2.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.2.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.2.input_layernorm.weight: requires_grad=True\n",
      "model.layers.2.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.3.self_attn.q_proj.weight: requires_grad=True\n",
      "model.layers.3.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.3.self_attn.v_proj.weight: requires_grad=True\n",
      "model.layers.3.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.3.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.3.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.3.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.3.input_layernorm.weight: requires_grad=True\n",
      "model.layers.3.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.4.self_attn.q_proj.weight: requires_grad=True\n",
      "model.layers.4.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.4.self_attn.v_proj.weight: requires_grad=True\n",
      "model.layers.4.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.4.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.4.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.4.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.4.input_layernorm.weight: requires_grad=True\n",
      "model.layers.4.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.5.self_attn.q_proj.weight: requires_grad=True\n",
      "model.layers.5.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.5.self_attn.v_proj.weight: requires_grad=True\n",
      "model.layers.5.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.5.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.5.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.5.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.5.input_layernorm.weight: requires_grad=True\n",
      "model.layers.5.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.6.self_attn.q_proj.weight: requires_grad=True\n",
      "model.layers.6.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.6.self_attn.v_proj.weight: requires_grad=True\n",
      "model.layers.6.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.6.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.6.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.6.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.6.input_layernorm.weight: requires_grad=True\n",
      "model.layers.6.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.7.self_attn.q_proj.weight: requires_grad=True\n",
      "model.layers.7.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.7.self_attn.v_proj.weight: requires_grad=True\n",
      "model.layers.7.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.7.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.7.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.7.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.7.input_layernorm.weight: requires_grad=True\n",
      "model.layers.7.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.8.self_attn.q_proj.weight: requires_grad=True\n",
      "model.layers.8.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.8.self_attn.v_proj.weight: requires_grad=True\n",
      "model.layers.8.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.8.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.8.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.8.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.8.input_layernorm.weight: requires_grad=True\n",
      "model.layers.8.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.9.self_attn.q_proj.weight: requires_grad=True\n",
      "model.layers.9.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.9.self_attn.v_proj.weight: requires_grad=True\n",
      "model.layers.9.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.9.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.9.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.9.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.9.input_layernorm.weight: requires_grad=True\n",
      "model.layers.9.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.10.self_attn.q_proj.weight: requires_grad=True\n",
      "model.layers.10.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.10.self_attn.v_proj.weight: requires_grad=True\n",
      "model.layers.10.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.10.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.10.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.10.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.10.input_layernorm.weight: requires_grad=True\n",
      "model.layers.10.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.11.self_attn.q_proj.weight: requires_grad=True\n",
      "model.layers.11.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.11.self_attn.v_proj.weight: requires_grad=True\n",
      "model.layers.11.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.11.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.11.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.11.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.11.input_layernorm.weight: requires_grad=True\n",
      "model.layers.11.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.12.self_attn.q_proj.weight: requires_grad=True\n",
      "model.layers.12.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.12.self_attn.v_proj.weight: requires_grad=True\n",
      "model.layers.12.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.12.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.12.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.12.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.12.input_layernorm.weight: requires_grad=True\n",
      "model.layers.12.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.13.self_attn.q_proj.weight: requires_grad=True\n",
      "model.layers.13.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.13.self_attn.v_proj.weight: requires_grad=True\n",
      "model.layers.13.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.13.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.13.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.13.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.13.input_layernorm.weight: requires_grad=True\n",
      "model.layers.13.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.14.self_attn.q_proj.weight: requires_grad=True\n",
      "model.layers.14.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.14.self_attn.v_proj.weight: requires_grad=True\n",
      "model.layers.14.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.14.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.14.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.14.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.14.input_layernorm.weight: requires_grad=True\n",
      "model.layers.14.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.15.self_attn.q_proj.weight: requires_grad=True\n",
      "model.layers.15.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.15.self_attn.v_proj.weight: requires_grad=True\n",
      "model.layers.15.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.15.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.15.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.15.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.15.input_layernorm.weight: requires_grad=True\n",
      "model.layers.15.post_attention_layernorm.weight: requires_grad=True\n",
      "model.norm.weight: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model1.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaForCausalLM' object has no attribute 'transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241m.\u001b[39mwte\n",
      "File \u001b[0;32m~/miniconda3/envs/my-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'transformer'"
     ]
    }
   ],
   "source": [
    "model1.transformer.wte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = AutoModelForCausalLM.from_pretrained(\"../model/QLoRA_distill_model\") \n",
    "model3 = AutoModelForCausalLM.from_pretrained(\"../model/distill_model\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForCausalLM                                        --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   (262,668,288)\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-1                --\n",
       "│    │    │    │    └─Linear: 5-1                       --\n",
       "│    │    │    │    │    └─Linear: 6-1                  (4,194,304)\n",
       "│    │    │    │    │    └─ModuleDict: 6-2              --\n",
       "│    │    │    │    │    └─ModuleDict: 6-3              (16,384)\n",
       "│    │    │    │    │    └─ModuleDict: 6-4              (16,384)\n",
       "│    │    │    │    │    └─ParameterDict: 6-5           --\n",
       "│    │    │    │    │    └─ParameterDict: 6-6           --\n",
       "│    │    │    │    │    └─ModuleDict: 6-7              --\n",
       "│    │    │    │    └─Linear: 5-2                       (1,048,576)\n",
       "│    │    │    │    └─Linear: 5-3                       --\n",
       "│    │    │    │    │    └─Linear: 6-8                  (1,048,576)\n",
       "│    │    │    │    │    └─ModuleDict: 6-9              --\n",
       "│    │    │    │    │    └─ModuleDict: 6-10             (16,384)\n",
       "│    │    │    │    │    └─ModuleDict: 6-11             (4,096)\n",
       "│    │    │    │    │    └─ParameterDict: 6-12          --\n",
       "│    │    │    │    │    └─ParameterDict: 6-13          --\n",
       "│    │    │    │    │    └─ModuleDict: 6-14             --\n",
       "│    │    │    │    └─Linear: 5-4                       (4,194,304)\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-5         --\n",
       "│    │    │    └─LlamaMLP: 4-2                          --\n",
       "│    │    │    │    └─Linear: 5-6                       (16,777,216)\n",
       "│    │    │    │    └─Linear: 5-7                       (16,777,216)\n",
       "│    │    │    │    └─Linear: 5-8                       (16,777,216)\n",
       "│    │    │    │    └─SiLU: 5-9                         --\n",
       "│    │    │    └─LlamaRMSNorm: 4-3                      (2,048)\n",
       "│    │    │    └─LlamaRMSNorm: 4-4                      (2,048)\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-5                --\n",
       "│    │    │    │    └─Linear: 5-10                      --\n",
       "│    │    │    │    │    └─Linear: 6-15                 (4,194,304)\n",
       "│    │    │    │    │    └─ModuleDict: 6-16             --\n",
       "│    │    │    │    │    └─ModuleDict: 6-17             (16,384)\n",
       "│    │    │    │    │    └─ModuleDict: 6-18             (16,384)\n",
       "│    │    │    │    │    └─ParameterDict: 6-19          --\n",
       "│    │    │    │    │    └─ParameterDict: 6-20          --\n",
       "│    │    │    │    │    └─ModuleDict: 6-21             --\n",
       "│    │    │    │    └─Linear: 5-11                      (1,048,576)\n",
       "│    │    │    │    └─Linear: 5-12                      --\n",
       "│    │    │    │    │    └─Linear: 6-22                 (1,048,576)\n",
       "│    │    │    │    │    └─ModuleDict: 6-23             --\n",
       "│    │    │    │    │    └─ModuleDict: 6-24             (16,384)\n",
       "│    │    │    │    │    └─ModuleDict: 6-25             (4,096)\n",
       "│    │    │    │    │    └─ParameterDict: 6-26          --\n",
       "│    │    │    │    │    └─ParameterDict: 6-27          --\n",
       "│    │    │    │    │    └─ModuleDict: 6-28             --\n",
       "│    │    │    │    └─Linear: 5-13                      (4,194,304)\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-14        --\n",
       "│    │    │    └─LlamaMLP: 4-6                          --\n",
       "│    │    │    │    └─Linear: 5-15                      (16,777,216)\n",
       "│    │    │    │    └─Linear: 5-16                      (16,777,216)\n",
       "│    │    │    │    └─Linear: 5-17                      (16,777,216)\n",
       "│    │    │    │    └─SiLU: 5-18                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-7                      (2,048)\n",
       "│    │    │    └─LlamaRMSNorm: 4-8                      (2,048)\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-9                --\n",
       "│    │    │    │    └─Linear: 5-19                      --\n",
       "│    │    │    │    │    └─Linear: 6-29                 (4,194,304)\n",
       "│    │    │    │    │    └─ModuleDict: 6-30             --\n",
       "│    │    │    │    │    └─ModuleDict: 6-31             (16,384)\n",
       "│    │    │    │    │    └─ModuleDict: 6-32             (16,384)\n",
       "│    │    │    │    │    └─ParameterDict: 6-33          --\n",
       "│    │    │    │    │    └─ParameterDict: 6-34          --\n",
       "│    │    │    │    │    └─ModuleDict: 6-35             --\n",
       "│    │    │    │    └─Linear: 5-20                      (1,048,576)\n",
       "│    │    │    │    └─Linear: 5-21                      --\n",
       "│    │    │    │    │    └─Linear: 6-36                 (1,048,576)\n",
       "│    │    │    │    │    └─ModuleDict: 6-37             --\n",
       "│    │    │    │    │    └─ModuleDict: 6-38             (16,384)\n",
       "│    │    │    │    │    └─ModuleDict: 6-39             (4,096)\n",
       "│    │    │    │    │    └─ParameterDict: 6-40          --\n",
       "│    │    │    │    │    └─ParameterDict: 6-41          --\n",
       "│    │    │    │    │    └─ModuleDict: 6-42             --\n",
       "│    │    │    │    └─Linear: 5-22                      (4,194,304)\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-23        --\n",
       "│    │    │    └─LlamaMLP: 4-10                         --\n",
       "│    │    │    │    └─Linear: 5-24                      (16,777,216)\n",
       "│    │    │    │    └─Linear: 5-25                      (16,777,216)\n",
       "│    │    │    │    └─Linear: 5-26                      (16,777,216)\n",
       "│    │    │    │    └─SiLU: 5-27                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-11                     (2,048)\n",
       "│    │    │    └─LlamaRMSNorm: 4-12                     (2,048)\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-13               --\n",
       "│    │    │    │    └─Linear: 5-28                      --\n",
       "│    │    │    │    │    └─Linear: 6-43                 (4,194,304)\n",
       "│    │    │    │    │    └─ModuleDict: 6-44             --\n",
       "│    │    │    │    │    └─ModuleDict: 6-45             (16,384)\n",
       "│    │    │    │    │    └─ModuleDict: 6-46             (16,384)\n",
       "│    │    │    │    │    └─ParameterDict: 6-47          --\n",
       "│    │    │    │    │    └─ParameterDict: 6-48          --\n",
       "│    │    │    │    │    └─ModuleDict: 6-49             --\n",
       "│    │    │    │    └─Linear: 5-29                      (1,048,576)\n",
       "│    │    │    │    └─Linear: 5-30                      --\n",
       "│    │    │    │    │    └─Linear: 6-50                 (1,048,576)\n",
       "│    │    │    │    │    └─ModuleDict: 6-51             --\n",
       "│    │    │    │    │    └─ModuleDict: 6-52             (16,384)\n",
       "│    │    │    │    │    └─ModuleDict: 6-53             (4,096)\n",
       "│    │    │    │    │    └─ParameterDict: 6-54          --\n",
       "│    │    │    │    │    └─ParameterDict: 6-55          --\n",
       "│    │    │    │    │    └─ModuleDict: 6-56             --\n",
       "│    │    │    │    └─Linear: 5-31                      (4,194,304)\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-32        --\n",
       "│    │    │    └─LlamaMLP: 4-14                         --\n",
       "│    │    │    │    └─Linear: 5-33                      (16,777,216)\n",
       "│    │    │    │    └─Linear: 5-34                      (16,777,216)\n",
       "│    │    │    │    └─Linear: 5-35                      (16,777,216)\n",
       "│    │    │    │    └─SiLU: 5-36                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-15                     (2,048)\n",
       "│    │    │    └─LlamaRMSNorm: 4-16                     (2,048)\n",
       "│    └─LlamaRMSNorm: 2-3                                (2,048)\n",
       "│    └─LlamaRotaryEmbedding: 2-4                        --\n",
       "├─Linear: 1-2                                           (262,668,288)\n",
       "================================================================================\n",
       "Total params: 768,837,632\n",
       "Trainable params: 0\n",
       "Non-trainable params: 768,837,632\n",
       "================================================================================"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model2, depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForCausalLM                                        --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   262,668,288\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-1                --\n",
       "│    │    │    │    └─Linear: 5-1                       4,194,304\n",
       "│    │    │    │    └─Linear: 5-2                       1,048,576\n",
       "│    │    │    │    └─Linear: 5-3                       1,048,576\n",
       "│    │    │    │    └─Linear: 5-4                       4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-5         --\n",
       "│    │    │    └─LlamaMLP: 4-2                          --\n",
       "│    │    │    │    └─Linear: 5-6                       16,777,216\n",
       "│    │    │    │    └─Linear: 5-7                       16,777,216\n",
       "│    │    │    │    └─Linear: 5-8                       16,777,216\n",
       "│    │    │    │    └─SiLU: 5-9                         --\n",
       "│    │    │    └─LlamaRMSNorm: 4-3                      2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-4                      2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-5                --\n",
       "│    │    │    │    └─Linear: 5-10                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-11                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-12                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-13                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-14        --\n",
       "│    │    │    └─LlamaMLP: 4-6                          --\n",
       "│    │    │    │    └─Linear: 5-15                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-16                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-17                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-18                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-7                      2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-8                      2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-9                --\n",
       "│    │    │    │    └─Linear: 5-19                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-20                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-21                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-22                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-23        --\n",
       "│    │    │    └─LlamaMLP: 4-10                         --\n",
       "│    │    │    │    └─Linear: 5-24                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-25                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-26                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-27                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-11                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-12                     2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      --\n",
       "│    │    │    └─LlamaSdpaAttention: 4-13               --\n",
       "│    │    │    │    └─Linear: 5-28                      4,194,304\n",
       "│    │    │    │    └─Linear: 5-29                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-30                      1,048,576\n",
       "│    │    │    │    └─Linear: 5-31                      4,194,304\n",
       "│    │    │    │    └─LlamaRotaryEmbedding: 5-32        --\n",
       "│    │    │    └─LlamaMLP: 4-14                         --\n",
       "│    │    │    │    └─Linear: 5-33                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-34                      16,777,216\n",
       "│    │    │    │    └─Linear: 5-35                      16,777,216\n",
       "│    │    │    │    └─SiLU: 5-36                        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-15                     2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-16                     2,048\n",
       "│    └─LlamaRMSNorm: 2-3                                2,048\n",
       "│    └─LlamaRotaryEmbedding: 2-4                        --\n",
       "├─Linear: 1-2                                           262,668,288\n",
       "================================================================================\n",
       "Total params: 768,624,640\n",
       "Trainable params: 768,624,640\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model3, depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight: requires_grad=False\n",
      "model.layers.0.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.0.self_attn.q_proj.lora_A.default.weight: requires_grad=False\n",
      "model.layers.0.self_attn.q_proj.lora_B.default.weight: requires_grad=False\n",
      "model.layers.0.self_attn.k_proj.weight: requires_grad=False\n",
      "model.layers.0.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.0.self_attn.v_proj.lora_A.default.weight: requires_grad=False\n",
      "model.layers.0.self_attn.v_proj.lora_B.default.weight: requires_grad=False\n",
      "model.layers.0.self_attn.o_proj.weight: requires_grad=False\n",
      "model.layers.0.mlp.gate_proj.weight: requires_grad=False\n",
      "model.layers.0.mlp.up_proj.weight: requires_grad=False\n",
      "model.layers.0.mlp.down_proj.weight: requires_grad=False\n",
      "model.layers.0.input_layernorm.weight: requires_grad=False\n",
      "model.layers.0.post_attention_layernorm.weight: requires_grad=False\n",
      "model.layers.1.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.1.self_attn.q_proj.lora_A.default.weight: requires_grad=False\n",
      "model.layers.1.self_attn.q_proj.lora_B.default.weight: requires_grad=False\n",
      "model.layers.1.self_attn.k_proj.weight: requires_grad=False\n",
      "model.layers.1.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.1.self_attn.v_proj.lora_A.default.weight: requires_grad=False\n",
      "model.layers.1.self_attn.v_proj.lora_B.default.weight: requires_grad=False\n",
      "model.layers.1.self_attn.o_proj.weight: requires_grad=False\n",
      "model.layers.1.mlp.gate_proj.weight: requires_grad=False\n",
      "model.layers.1.mlp.up_proj.weight: requires_grad=False\n",
      "model.layers.1.mlp.down_proj.weight: requires_grad=False\n",
      "model.layers.1.input_layernorm.weight: requires_grad=False\n",
      "model.layers.1.post_attention_layernorm.weight: requires_grad=False\n",
      "model.layers.2.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.2.self_attn.q_proj.lora_A.default.weight: requires_grad=False\n",
      "model.layers.2.self_attn.q_proj.lora_B.default.weight: requires_grad=False\n",
      "model.layers.2.self_attn.k_proj.weight: requires_grad=False\n",
      "model.layers.2.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.2.self_attn.v_proj.lora_A.default.weight: requires_grad=False\n",
      "model.layers.2.self_attn.v_proj.lora_B.default.weight: requires_grad=False\n",
      "model.layers.2.self_attn.o_proj.weight: requires_grad=False\n",
      "model.layers.2.mlp.gate_proj.weight: requires_grad=False\n",
      "model.layers.2.mlp.up_proj.weight: requires_grad=False\n",
      "model.layers.2.mlp.down_proj.weight: requires_grad=False\n",
      "model.layers.2.input_layernorm.weight: requires_grad=False\n",
      "model.layers.2.post_attention_layernorm.weight: requires_grad=False\n",
      "model.layers.3.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.3.self_attn.q_proj.lora_A.default.weight: requires_grad=False\n",
      "model.layers.3.self_attn.q_proj.lora_B.default.weight: requires_grad=False\n",
      "model.layers.3.self_attn.k_proj.weight: requires_grad=False\n",
      "model.layers.3.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.3.self_attn.v_proj.lora_A.default.weight: requires_grad=False\n",
      "model.layers.3.self_attn.v_proj.lora_B.default.weight: requires_grad=False\n",
      "model.layers.3.self_attn.o_proj.weight: requires_grad=False\n",
      "model.layers.3.mlp.gate_proj.weight: requires_grad=False\n",
      "model.layers.3.mlp.up_proj.weight: requires_grad=False\n",
      "model.layers.3.mlp.down_proj.weight: requires_grad=False\n",
      "model.layers.3.input_layernorm.weight: requires_grad=False\n",
      "model.layers.3.post_attention_layernorm.weight: requires_grad=False\n",
      "model.norm.weight: requires_grad=False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model2.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model2.named_parameters():\n",
    "    if not 'base' in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight: requires_grad=True\n",
      "model.layers.0.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.0.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.0.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.0.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.0.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.0.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.0.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.0.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.0.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.0.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.0.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.0.input_layernorm.weight: requires_grad=True\n",
      "model.layers.0.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.1.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.1.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.1.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.1.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.1.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.1.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.1.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.1.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.1.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.1.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.1.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.1.input_layernorm.weight: requires_grad=True\n",
      "model.layers.1.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.2.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.2.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.2.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.2.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.2.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.2.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.2.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.2.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.2.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.2.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.2.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.2.input_layernorm.weight: requires_grad=True\n",
      "model.layers.2.post_attention_layernorm.weight: requires_grad=True\n",
      "model.layers.3.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.3.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.3.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.3.self_attn.k_proj.weight: requires_grad=True\n",
      "model.layers.3.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "model.layers.3.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "model.layers.3.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "model.layers.3.self_attn.o_proj.weight: requires_grad=True\n",
      "model.layers.3.mlp.gate_proj.weight: requires_grad=True\n",
      "model.layers.3.mlp.up_proj.weight: requires_grad=True\n",
      "model.layers.3.mlp.down_proj.weight: requires_grad=True\n",
      "model.layers.3.input_layernorm.weight: requires_grad=True\n",
      "model.layers.3.post_attention_layernorm.weight: requires_grad=True\n",
      "model.norm.weight: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model2.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = AutoModelForCausalLM.from_pretrained(\"\") \n",
    "from torchinfo import summary\n",
    "summary(model1, depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "# BERT Base のインポート\n",
    "bert_base_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# DistilBERT のインポート\n",
    "distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "BertModel                                               --\n",
       "├─BertEmbeddings: 1-1                                   --\n",
       "│    └─Embedding: 2-1                                   23,440,896\n",
       "│    └─Embedding: 2-2                                   393,216\n",
       "│    └─Embedding: 2-3                                   1,536\n",
       "│    └─LayerNorm: 2-4                                   1,536\n",
       "│    └─Dropout: 2-5                                     --\n",
       "├─BertEncoder: 1-2                                      --\n",
       "│    └─ModuleList: 2-6                                  --\n",
       "│    │    └─BertLayer: 3-1                              7,087,872\n",
       "│    │    └─BertLayer: 3-2                              7,087,872\n",
       "│    │    └─BertLayer: 3-3                              7,087,872\n",
       "│    │    └─BertLayer: 3-4                              7,087,872\n",
       "│    │    └─BertLayer: 3-5                              7,087,872\n",
       "│    │    └─BertLayer: 3-6                              7,087,872\n",
       "│    │    └─BertLayer: 3-7                              7,087,872\n",
       "│    │    └─BertLayer: 3-8                              7,087,872\n",
       "│    │    └─BertLayer: 3-9                              7,087,872\n",
       "│    │    └─BertLayer: 3-10                             7,087,872\n",
       "│    │    └─BertLayer: 3-11                             7,087,872\n",
       "│    │    └─BertLayer: 3-12                             7,087,872\n",
       "├─BertPooler: 1-3                                       --\n",
       "│    └─Linear: 2-7                                      590,592\n",
       "│    └─Tanh: 2-8                                        --\n",
       "================================================================================\n",
       "Total params: 109,482,240\n",
       "Trainable params: 109,482,240\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(bert_base_model, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "DistilBertModel                                    --\n",
       "├─Embeddings: 1-1                                  --\n",
       "│    └─Embedding: 2-1                              23,440,896\n",
       "│    └─Embedding: 2-2                              393,216\n",
       "│    └─LayerNorm: 2-3                              1,536\n",
       "│    └─Dropout: 2-4                                --\n",
       "├─Transformer: 1-2                                 --\n",
       "│    └─ModuleList: 2-5                             --\n",
       "│    │    └─TransformerBlock: 3-1                  7,087,872\n",
       "│    │    └─TransformerBlock: 3-2                  7,087,872\n",
       "│    │    └─TransformerBlock: 3-3                  7,087,872\n",
       "│    │    └─TransformerBlock: 3-4                  7,087,872\n",
       "│    │    └─TransformerBlock: 3-5                  7,087,872\n",
       "│    │    └─TransformerBlock: 3-6                  7,087,872\n",
       "===========================================================================\n",
       "Total params: 66,362,880\n",
       "Trainable params: 66,362,880\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(distilbert_model, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
