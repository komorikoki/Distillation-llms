{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/my-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 21/21 [02:34<00:00,  7.34s/files]\n",
      "Generating train split: 100%|██████████| 8013769/8013769 [18:24<00:00, 7255.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# OpenWebTextデータセットのロード\n",
    "dataset = load_dataset('openwebtext')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# OpenWebTextデータセットを読み込む\n",
    "dataset = load_dataset('openwebtext')\n",
    "\n",
    "# 'train'セットから5000サンプルを取得\n",
    "limited_dataset = dataset['train'].select(range(5000))\n",
    "\n",
    "# データの確認\n",
    "print(limited_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# バッチサイズを指定\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# デバイスの設定\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# トークナイザーと教師モデルをロード\n",
    "teacher_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "student_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "teacher_model.to(device)  # GPUを使用する場合\n",
    "student_model.to(device)\n",
    "\n",
    "# トークナイザーをロード\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss(student_logits, teacher_logits, temperature=2.0):\n",
    "    teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "    student_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
    "    return F.kl_div(student_probs, teacher_probs) * (temperature ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Progress:   0%|          | 0/5000 [00:00<?, ?it/s]/root/miniconda3/envs/my-env/lib/python3.9/site-packages/torch/nn/functional.py:3369: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 5.995044460860299e-08\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 1.1594470095133147e-07\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 1.8289404124516295e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# オプティマイザーの設定\n",
    "optimizer = optim.AdamW(student_model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 3  # エポック数\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    epoch_progress = tqdm(limited_dataset, desc=\"Batch Progress\", leave=False)\n",
    "    \n",
    "    for example in epoch_progress:\n",
    "        input_text = example['text']\n",
    "        inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True).to('cuda')\n",
    "\n",
    "        # 教師モデルの出力を取得\n",
    "        with torch.no_grad():\n",
    "            teacher_output = teacher_model(**inputs)\n",
    "            teacher_logits = teacher_output.logits\n",
    "\n",
    "        # 生徒モデルの出力を取得\n",
    "        student_output = student_model(**inputs)\n",
    "        student_logits = student_output.logits\n",
    "\n",
    "        # 蒸留損失の計算\n",
    "        loss = distillation_loss(student_logits, teacher_logits)\n",
    "        \n",
    "        # 勾配計算とオプティマイザーのステップ\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 各バッチの損失を進捗バーに表示\n",
    "        epoch_progress.set_postfix({\"Loss\": loss.item()})\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニング後のモデルを保存\n",
    "student_model.save_pretrained('mistral_distilled_model')\n",
    "student_tokenizer.save_pretrained('mistral_distilled_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
