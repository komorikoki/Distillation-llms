{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Distilledモデルとトークナイザーをロード\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "device=\"cuda\"\n",
    "model.to(device)\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_text, model, tokenizer, max_length=100, temperature=1.0):\n",
    "    \n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=100).to(\"cuda\")  # 必要に応じてCUDAに送る\n",
    "    output = model.generate(inputs[\"input_ids\"], attention_mask=inputs['attention_mask'], max_length=max_length, num_return_sequences=1, temperature=temperature)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[5661,  318,  257]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1]], device='cuda:0')}\n",
      "tensor([[1, 1, 1]], device='cuda:0')\n",
      "tensor([[5661,  318,  257,  845,  922, 2126,  284,  779,  257, 1180, 3124, 7791,\n",
      "          329,  262,  976, 1517,   13,  198,  198,  464,  717, 1517,  345,  761,\n",
      "          284,  466,  318,  284,  751,  257,  649, 1627,  284,  262,  886,  286,\n",
      "          262, 1627,   13,  198]], device='cuda:0')\n",
      "this is a very good idea to use a different color scheme for the same thing.\n",
      "\n",
      "The first thing you need to do is to add a new line to the end of the line.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([\"this is a\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=20).to(\"cuda\") \n",
    "print(inputs)\n",
    "attention_mask=inputs['attention_mask']\n",
    "print(attention_mask)\n",
    "output = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=40, num_return_sequences=1, temperature=1.0 )\n",
    "print(output)\n",
    "response = tokenizer.decode(output[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is your name ?\\n\\n\"I\\'m a little bit of a mystery to you, but I\\'m a little bit of a mystery to you, too.\"\\n\\n\"I\\'m a little bit of a mystery to you, too.\"\\n\\n\"I\\'m a little bit of a mystery to you, too.\"\\n\\n\"I\\'m a little bit of a mystery to you, too.\"\\n\\n\"I\\'m a little bit of a mystery to you, too.\"\\n\\n\"'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = [\"what is your name ?\"]\n",
    "generate_response(user_input, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1文目のtokenize結果:  tensor([[20015,   232, 33768,    98, 31676, 25465, 36365,   245, 35585,   164,\n",
      "           231,   107, 18566,  5641, 30640, 13783,   244, 28618, 49035,   118,\n",
      "         27370,  2515,   239, 25224, 18566, 26945, 16764]])\n",
      "2文目のtokenize結果:  tensor([[20015,   232, 33768,    98, 31676,   162,   248,   239, 18566, 16764]])\n",
      "入力バッチのtokenize結果 tensor([[20015,   232, 33768,    98, 31676, 25465, 36365,   245, 35585,   164,\n",
      "           231,   107, 18566,  5641, 30640, 13783,   244, 28618, 49035,   118,\n",
      "         27370,  2515,   239, 25224, 18566, 26945, 16764],\n",
      "        [20015,   232, 33768,    98, 31676,   162,   248,   239, 18566, 16764,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "sequences = ['今日は天気が良いので外に出かけたいな。', '今日は暑い。']\n",
    "\n",
    "sequence1_ids = tokenizer(sequences[0], return_tensors='pt').input_ids\n",
    "sequence2_ids = tokenizer(sequences[1], return_tensors='pt').input_ids\n",
    "\n",
    "print(\"1文目のtokenize結果: \", sequence1_ids) # 1文目のtokenize結果\n",
    "print(\"2文目のtokenize結果: \", sequence2_ids) # ２文目のtokenize結果\n",
    "\n",
    "# 入力をpaddingしてバッチにする\n",
    "batched_ids = tokenizer(sequences, padding=True, return_tensors='pt').input_ids # padding=Trueで自動的にPaddingしてくれる\n",
    "\n",
    "print(\"入力バッチのtokenize結果\", batched_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['近年人工知能の活用は著しく上昇している。</s>人工知能(ai)は、人間の脳と似ている部分があることがわかってきた。「コンピューターで人間', '近年人工知能の活用は著しく上昇している。</s>人工知能が人間の知的能力を大きく凌駕するほどのスピードで向上することを人間は受け入れてはおらず', '近年人工知能の活用は著しく上昇している。</s>人工知能の進化は、今後、あらゆる分野における重要なスキルとして発展し、またそれを支える情報インフラ']\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, AutoModelForCausalLM\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
    "\n",
    "# load pre-trained model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
    "\n",
    "# Set input word\n",
    "input = tokenizer.encode(\"近年人工知能の活用は著しく上昇している。\", return_tensors=\"pt\")\n",
    "\n",
    "# inference\n",
    "output = model.generate(input, do_sample=True, max_length=30, num_return_sequences=3)\n",
    "\n",
    "# inferred output\n",
    "print(tokenizer.batch_decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
