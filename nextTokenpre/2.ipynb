{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "device='cuda'\n",
    "# モデルの準備\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=ds[\"train\"].shuffle(seed=42).select(range(20000))\n",
    "validation_dataset=ds[\"validation\"].shuffle(seed=42).select(range(3500))\n",
    "train_dataset = train_dataset[\"text\"]\n",
    "train_dataset = [item for item in train_dataset if item != '' and len(item) >= 50 and '@' not in item]\n",
    "validation_dataset=validation_dataset[\"text\"]\n",
    "validation_dataset = [item for item in validation_dataset if item != '' and len(item) >= 50 and '@' not in item]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "train_dataset = [re.sub(r'[^a-zA-Z ]', '', item) for item in train_dataset]\n",
    "train_dataset = [re.sub(r'\\s+', ' ', item) for item in train_dataset]\n",
    "validation_dataset = [re.sub(r'[^a-zA-Z ]', '', item) for item in validation_dataset]\n",
    "validation_dataset = [re.sub(r'\\s+', ' ', item) for item in validation_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000,\n",
       "  9220,\n",
       "  4106,\n",
       "  47497,\n",
       "  430,\n",
       "  34234,\n",
       "  25024,\n",
       "  374,\n",
       "  11033,\n",
       "  10434,\n",
       "  304,\n",
       "  279,\n",
       "  6693,\n",
       "  323,\n",
       "  6957,\n",
       "  279,\n",
       "  1938,\n",
       "  323,\n",
       "  15600,\n",
       "  902,\n",
       "  374,\n",
       "  3629,\n",
       "  274,\n",
       "  6586,\n",
       "  304,\n",
       "  279,\n",
       "  11714,\n",
       "  31125,\n",
       "  374,\n",
       "  6118,\n",
       "  88340,\n",
       "  449,\n",
       "  4415,\n",
       "  41390,\n",
       "  29437,\n",
       "  477,\n",
       "  296,\n",
       "  640,\n",
       "  72,\n",
       "  4912,\n",
       "  2701,\n",
       "  39361,\n",
       "  578,\n",
       "  11033,\n",
       "  315,\n",
       "  5873,\n",
       "  374,\n",
       "  6118,\n",
       "  24666,\n",
       "  477,\n",
       "  35217,\n",
       "  11033,\n",
       "  35217,\n",
       "  11033,\n",
       "  374,\n",
       "  4528,\n",
       "  311,\n",
       "  24666,\n",
       "  11033,\n",
       "  719,\n",
       "  279,\n",
       "  4846,\n",
       "  374,\n",
       "  993,\n",
       "  7725,\n",
       "  449,\n",
       "  3786,\n",
       "  309,\n",
       "  316,\n",
       "  323,\n",
       "  374,\n",
       "  6118,\n",
       "  7120,\n",
       "  4589,\n",
       "  6901,\n",
       "  220,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009],\n",
       " 'labels': [9220,\n",
       "  4106,\n",
       "  47497,\n",
       "  430,\n",
       "  34234,\n",
       "  25024,\n",
       "  374,\n",
       "  11033,\n",
       "  10434,\n",
       "  304,\n",
       "  279,\n",
       "  6693,\n",
       "  323,\n",
       "  6957,\n",
       "  279,\n",
       "  1938,\n",
       "  323,\n",
       "  15600,\n",
       "  902,\n",
       "  374,\n",
       "  3629,\n",
       "  274,\n",
       "  6586,\n",
       "  304,\n",
       "  279,\n",
       "  11714,\n",
       "  31125,\n",
       "  374,\n",
       "  6118,\n",
       "  88340,\n",
       "  449,\n",
       "  4415,\n",
       "  41390,\n",
       "  29437,\n",
       "  477,\n",
       "  296,\n",
       "  640,\n",
       "  72,\n",
       "  4912,\n",
       "  2701,\n",
       "  39361,\n",
       "  578,\n",
       "  11033,\n",
       "  315,\n",
       "  5873,\n",
       "  374,\n",
       "  6118,\n",
       "  24666,\n",
       "  477,\n",
       "  35217,\n",
       "  11033,\n",
       "  35217,\n",
       "  11033,\n",
       "  374,\n",
       "  4528,\n",
       "  311,\n",
       "  24666,\n",
       "  11033,\n",
       "  719,\n",
       "  279,\n",
       "  4846,\n",
       "  374,\n",
       "  993,\n",
       "  7725,\n",
       "  449,\n",
       "  3786,\n",
       "  309,\n",
       "  316,\n",
       "  323,\n",
       "  374,\n",
       "  6118,\n",
       "  7120,\n",
       "  4589,\n",
       "  6901,\n",
       "  220,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009,\n",
       "  128009]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[665]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力とラベルを設定\n",
    "validation_data = []\n",
    "for text in validation_dataset:\n",
    "    tokenized = tokenizer(text, padding=\"max_length\", max_length=128, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = tokenized['input_ids'].squeeze().tolist()\n",
    "    # 次の単語のインデックスをラベルとして追加\n",
    "    labels = input_ids[1:] + [tokenizer.pad_token_id]  # 最初の単語を除いて次の単語をラベルにする\n",
    "    validation_data.append({\"input_ids\": input_ids, \"labels\": labels})\n",
    "\n",
    "# Datasetの作成\n",
    "validation_dataset = Dataset.from_list(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset: 100%|██████████| 4063/4063 [00:01<00:00, 3688.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "# 入力とラベルを設定\n",
    "train_data = []\n",
    "for text in tqdm(train_dataset, desc=\"Tokenizing dataset\"):\n",
    "    tokenized = tokenizer(text, padding=\"max_length\", max_length=128, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = tokenized['input_ids'].squeeze().tolist()\n",
    "    # 次の単語のインデックスをラベルとして追加\n",
    "    labels = input_ids[1:] + [tokenizer.pad_token_id]  # 最初の単語を除いて次の単語をラベルにする\n",
    "    train_data.append({\"input_ids\": input_ids, \"labels\": labels})\n",
    "\n",
    "# Datasetの作成\n",
    "train_dataset = Dataset.from_list(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 10771, 311, 279, 5165, 6017, 14821, 315, 25431, 1369, 809, 285, 1101, 11224, 430, 40424, 462, 88, 77383, 3235, 449, 38577, 3714, 32743, 2836, 309, 38966, 374, 832, 315, 279, 1403, 12474, 36467, 315, 6617, 18341, 61801, 220, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009]\n",
      "[10771, 311, 279, 5165, 6017, 14821, 315, 25431, 1369, 809, 285, 1101, 11224, 430, 40424, 462, 88, 77383, 3235, 449, 38577, 3714, 32743, 2836, 309, 38966, 374, 832, 315, 279, 1403, 12474, 36467, 315, 6617, 18341, 61801, 220, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset['input_ids'][10])\n",
    "print(train_dataset['labels'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "teacher_model.to(device)\n",
    "# トレーニング設定\n",
    "teacher_model.train()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trainteacher\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    ")\n",
    "\n",
    "# Trainerのセットアップ\n",
    "trainer = Trainer(\n",
    "    model=teacher_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='5080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  51/5080 02:20 < 4:00:33, 0.35 it/s, Epoch 0.05/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 教師モデルのトレーニング\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m teacher_model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./teacherlg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/my-env/lib/python3.9/site-packages/transformers/trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2120\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2123\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/my-env/lib/python3.9/site-packages/transformers/trainer.py:2426\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2424\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2425\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2426\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs \u001b[38;5;129;01min\u001b[39;00m batch_samples:\n\u001b[1;32m   2428\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/my-env/lib/python3.9/site-packages/transformers/trainer.py:5038\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5036\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5038\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m   5039\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5040\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/my-env/lib/python3.9/site-packages/accelerate/data_loader.py:559\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 559\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[1;32m    561\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "File \u001b[0;32m~/miniconda3/envs/my-env/lib/python3.9/site-packages/accelerate/utils/operations.py:184\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[0;32m--> 184\u001b[0m         {\n\u001b[1;32m    185\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking, skip_keys\u001b[38;5;241m=\u001b[39mskip_keys)\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    187\u001b[0m         }\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/miniconda3/envs/my-env/lib/python3.9/site-packages/accelerate/utils/operations.py:185\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[1;32m    184\u001b[0m         {\n\u001b[0;32m--> 185\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    187\u001b[0m         }\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/miniconda3/envs/my-env/lib/python3.9/site-packages/accelerate/utils/operations.py:156\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    154\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 教師モデルのトレーニング\n",
    "trainer.train()\n",
    "teacher_model.save_pretrained(\"./teacherlg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.124519348144531, 'eval_model_preparation_time': 0.0019, 'eval_runtime': 4.0364, 'eval_samples_per_second': 47.568, 'eval_steps_per_second': 5.946}\n"
     ]
    }
   ],
   "source": [
    "teacher_model.eval()\n",
    "eval_results=trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model.push_to_hub(\"llama-3epochs\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
