{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "device='cuda'\n",
    "# モデルの準備}\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\"./instruction_model3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "# teacher_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "teacher_model.eval()\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[[128000,\n",
    " 362,\n",
    " 10007,\n",
    " 75662,\n",
    " 2543,]]\n",
    "attention_mask=[1,1,1,1,1]\n",
    "labels=[362,10007, 75662, 2543, 128000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs=torch.tensor(inputs)\n",
    "labels=torch.tensor(labels)\n",
    "attention_mask=torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Q:What is the most big number times small number? 40 90 92 90 20 93 91 97 92 90 A:\"\n",
    "tokenized = tokenizer(text, padding=\"longest\", max_length=100, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = teacher_model(input_ids=tokenized['input_ids'], attention_mask=tokenized['attention_mask'], labels=tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  48,   25,  362,  374,  279, 1187, 1187, 1187, 1187, 1187, 1187, 1187,\n",
      "          975, 1187,   13,  966,   13,  966,   13, 1135,   13, 1135,   13, 1135,\n",
      "           13, 1135,   13,   15,   13,   15,   13,   15,   13,   25,   15]])\n"
     ]
    }
   ],
   "source": [
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "most_likely_token = torch.argmax(probabilities[:, :, :], dim=-1)\n",
    "print(most_likely_token)\n",
    "most_likely_token = tokenizer.decode(most_likely_token[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q: A is the242424242424241424.30.30.50.50.50.50.0.0.0.:0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_likely_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer))  # トークナイザーのサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
