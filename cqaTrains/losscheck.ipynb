{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "ds = load_dataset(\"rajpurkar/squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"../distillLLAMA2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=10\n",
    "batch=1\n",
    "train_dataset=ds[\"train\"].shuffle(seed=42).select(range(size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_data=[]\n",
    "for i in range(size):\n",
    "    tokenized = tokenizer(train_dataset[i]['context']+train_dataset[i]['question'] + train_dataset[i]['answers']['text'][0], padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    cqlen = len(tokenizer(train_dataset[i]['context']+train_dataset[i]['question'])['input_ids'])\n",
    "    length=torch.sum(tokenized['attention_mask'][0])\n",
    "    input_ids = tokenized['input_ids'].squeeze().tolist()\n",
    "    attention_mask = tokenized['attention_mask'].squeeze().tolist()\n",
    "    labels = input_ids[1:] + [tokenizer.pad_token_id]\n",
    "    for i in range(len(attention_mask)):\n",
    "        if attention_mask[i]==0:\n",
    "            labels[i]=-100\n",
    "\n",
    "    # labels=[-100]*512\n",
    "    # for i in range(length-cqlen):\n",
    "    #     labels[cqlen+i-1]=input_ids[cqlen+i]\n",
    "    train_data.append({\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\":attention_mask})\n",
    "\n",
    "sbatch = int(size/batch)\n",
    "\n",
    "\n",
    "input_ids = [item[\"input_ids\"] for item in train_data]\n",
    "labels = [item[\"labels\"] for item in train_data]\n",
    "attention_mask = [item[\"attention_mask\"] for item in train_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "device='cuda'\n",
    "input_ids_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "attention_mask_tensor = torch.tensor(attention_mask, dtype=torch.long)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "criterion.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "input_ids_tensor=input_ids_tensor.to(device)\n",
    "labels_tensor=labels_tensor.to(device)\n",
    "attention_mask_tensor = attention_mask_tensor.to(device)\n",
    "model.to(device)\n",
    "eval_loss=0\n",
    "model.train()\n",
    "\n",
    "eval_losses = []\n",
    "vocab_size = model.config.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=input_ids_tensor[0].unsqueeze(0)\n",
    "labels=labels_tensor[0].unsqueeze(0)\n",
    "attention_mask=attention_mask_tensor[0].unsqueeze(0)\n",
    "optimizer.zero_grad()\n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "logits=outputs.logits\n",
    "loss = criterion(logits.view(-1, vocab_size), labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.3120, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
